{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This is a playground for developing the static website scraper. Functions developed here will be eventually packaged into .py files.\n",
    "\n",
    "\n",
    "**Progress**\n",
    "\n",
    "Done:\n",
    "    1. get response function\n",
    "    2. element finder and extractor (parser)\n",
    "    3. file saver\n",
    "\n",
    "To-do:\n",
    "    4. pagination\n",
    "    5. rotate agent\n",
    "    6. rotate IP\n",
    "    \n",
    "Long-term:\n",
    "    - support authentication \n",
    "    - support cookie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure you install the required libraries\n",
    "\n",
    "# !pip3 install --upgrade multipledispatch # library for overloading functions\n",
    "# !pip3 install --upgrade requests # library for making request for the static websites\n",
    "# !pip3 install --upgrade soupsieve  # library to support css selector in beautifulsoup\n",
    "# !pip3 install --upgrade beautifulsoup4 # a parser that balances between efficiency and leniency\n",
    "# !pip3 install --upgrade --user lxml # a more efficient parser\n",
    "# !pip3 install --upgrade html5lib # a parser that acts like a browser, most lenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key libraries\n",
    "import re\n",
    "import os\n",
    "import inspect\n",
    "\n",
    "from multipledispatch import dispatch\n",
    "\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions help us understand the variables that exist in the environment\n",
    "# which is useful for creating natural language interface for data analysis\n",
    "\n",
    "def get_local_variables(ignore_underscore = True):\n",
    "    \"\"\"Get the name and definition of the local variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dictionary\n",
    "        A mapping between name and definition of the local variables.\n",
    "                \n",
    "    \"\"\"\n",
    "    callers_local_vars = dict(inspect.currentframe().f_back.f_locals.items())\n",
    "    if filter_:\n",
    "        var_keys = list(callers_local_vars.keys())\n",
    "        for key in var_keys:\n",
    "            if key.startswith('_'):\n",
    "                del callers_local_vars[key]\n",
    "    return callers_local_vars\n",
    "def retrieve_name(var):\n",
    "    \"\"\"Retrieve the name of the variable. # Reference https://stackoverflow.com/a/40536047.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var: object \n",
    "        Variable to get the name of.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        Name of the variable passed.\n",
    "        \n",
    "    \"\"\"\n",
    "    for fi in reversed(inspect.stack()):\n",
    "        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "        if len(names) > 0:\n",
    "            return names[0]\n",
    "        \n",
    "def get_attributes(obj, ignore_underscore = True):\n",
    "    \"\"\"Get a list of valid attributes of the object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A list of valid attributes of the object.\n",
    "                \n",
    "    \"\"\"\n",
    "    return [x for x in dir(obj) if not x.startswith('_')]\n",
    "\n",
    "def print_attributes_and_values(obj, ignore_underscore = True):\n",
    "    \"\"\"Print the valid attributes of the object and their corresponding values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "                \n",
    "    \"\"\"\n",
    "    obj_name = retrieve_name(obj)\n",
    "    attributes = get_attributes(obj, ignore_underscore = ignore_underscore)\n",
    "    for attr in attributes:\n",
    "        obj_attr_string = obj_name+'.'+attr\n",
    "        print(obj_attr_string)\n",
    "        print(' '*4 + str(eval(obj_attr_string))[:60])\n",
    "        print('-'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: get response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests.get` function takes the following parameters:\n",
    "\n",
    " - url – URL for the new  Request object.\n",
    " - params – (optional) Dictionary of GET Parameters to send with the Request.\n",
    " - headers – (optional) Dictionary of HTTP Headers to send with the Request.\n",
    " - cookies – (optional) CookieJar object to send with the  Request.\n",
    " - auth – (optional) AuthObject to enable Basic HTTP Auth.\n",
    " - timeout – (optional) Float describing the timeout of the request.\n",
    " \n",
    "Note the requests made by this function could be recognized as robotic access by some website. To bypass screening by those websites, additional specifications on headers and proxies are required. These additional setup will be implemented in the future versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(str)\n",
    "def get_response(url, verbose = True):\n",
    "    \"\"\"Get the response of the HTTP GET request for the target url.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        The url to the website that needs to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    response object\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise Exception when response was not successful\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print('[Error] HTTP error occurred: '+str(http_err))\n",
    "        return requests.models.Response() # Return empty response\n",
    "    except Exception as err:\n",
    "        print('[Error] Other error occurred: '+str(err))\n",
    "        return requests.models.Response() # Return empty response\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('[Success] The website at \"'+url+'\" is collected succesfully.')\n",
    "        return response\n",
    "\n",
    "@dispatch(list)\n",
    "def get_response(urls, verbose = True):\n",
    "    \"\"\"Get the responses of the HTTP GET requests for the target urls. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    urls: list of string\n",
    "        The urls to the websites that need to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of response object\n",
    "        \n",
    "    \"\"\"\n",
    "    return [get_response(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Success and failure examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"http://google.com\" is collected succesfully.\n"
     ]
    }
   ],
   "source": [
    "response = get_response('http://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] Other error occurred: HTTPConnectionPool(host='somewebsitethatdontexist.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10d042a20>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))\n"
     ]
    }
   ],
   "source": [
    "response = get_response('http://somewebsitethatdontexist.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"http://google.com\" is collected succesfully.\n",
      "[Success] The website at \"https://baidu.com\" is collected succesfully.\n",
      "[Error] Other error occurred: HTTPSConnectionPool(host='anotherwebsitethatdontexist.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x10d073e10>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))\n"
     ]
    }
   ],
   "source": [
    "response_list = get_response(['http://google.com','https://baidu.com','https://anotherwebsitethatdontexist.com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example to use throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://digitalcollections.nypl.org'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"https://digitalcollections.nypl.org\" is collected succesfully.\n"
     ]
    }
   ],
   "source": [
    "response = get_response(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: parse response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the content of the website is collected, the next step is to parse the page. After parsing, we can find the elements we want, then extract and clean their values. For this step, there are many choices of libraries, some examples are: \n",
    "\n",
    " - BeautifulSoup\n",
    " - Scrapy\n",
    " - Lxml\n",
    " - AdvancedHTMLParser\n",
    " \n",
    "In this pipeline, we will explore `BeautifulSoup` and `Scrapy` for parsing, because these two are relatively more popular and have rich resources. Between these two, BeautifulSoup is more user-friendly, while Scrapy is more efficient and scalable.\n",
    "\n",
    "The other two libraries listed above are good choices in their specific areas, so keep them in view:\n",
    " - Lxml has rich features for processing XML and HTML and is quite efficient (BeautifulSoup actually supports using Lxml parser among other parsers).\n",
    " - AdvancedHTMLParser has similar functions like in native JavaScript and supports complex operations on HTML.\n",
    "  \n",
    "References:\n",
    " - https://smartproxy.com/blog/scrapy-vs-beautifulsoup (Use cases comparison and Pros&Cons)\n",
    " - https://tomassetti.me/parsing-html (Common libraries in different programming languages)\n",
    " - https://medium.com/analytics-vidhya/scrapy-vs-selenium-vs-beautiful-soup-for-web-scraping-24008b6c87b8 (Great comparison article that includes Selenium, which is the popular choice for dynamic website scraping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup\n",
    "\n",
    "References:   \n",
    " - https://www.datacamp.com/community/tutorials/amazon-web-scraping-using-beautifulsoup (Showed how to write element finding logic in hierarchy)\n",
    " - https://stackabuse.com/guide-to-parsing-html-with-beautifulsoup-in-python (Nice illustrations, browse_and_scrape combines pagination with parsing) \n",
    " - https://www.crummy.com/software/BeautifulSoup/bs4/doc (Long but detailed description of BS4 usage)\n",
    " - https://www.crummy.com/software/BeautifulSoup (The \"Hall of Fame\" section has some high-profile projects, worth having a look)\n",
    " \n",
    " \n",
    "An interesting side note, here one quote from the BS project page:\n",
    "\n",
    "> You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\n",
    "\n",
    "But actually, you cannot directly ask BS these natural language questions. You need to write codes that follow the syntax of the BS4 library, which is similar but not quite close to natural language. **Programming with natural language** is one of the directions worth pursuing in the future, as it further lowers the bar for utilizing web scraping and related technologies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- find_all\n",
    "- select\n",
    "- encodings (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#unicode-dammit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(response, default_parser = 'lxml'):\n",
    "    \"\"\"Get the beautiful soup object of the response object or filepath or html string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.models.Response, string\n",
    "        The response object or filepath or html string. \n",
    "    default_parser: string (optional, default = lxml)\n",
    "        Which parser to use when parsing the response.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of response object\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(response, requests.models.Response):\n",
    "        soup = bs4.BeautifulSoup(response.content, default_parser)\n",
    "    elif isinstance(response, str) and os.path.exists(response):\n",
    "        with open(response) as file_handler:\n",
    "            soup = bs4.BeautifulSoup(file_handler, default_parser)\n",
    "    else:\n",
    "        try:\n",
    "            soup = bs4.BeautifulSoup(response, default_parser)\n",
    "        except Exception as err:\n",
    "            print('[Error] The response object you provided cannot be turned into beautiful soup object: '+str(err))\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(html_object, path = './TEMP.html'):\n",
    "    \"\"\"Save the response or soup object as a HTML file at the path provided.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    html_object: requests.models.Response, bs4.BeautifulSoup\n",
    "        The response or soup object. \n",
    "    path: string (optional, default = ./TEMP.html)\n",
    "        The path at which the HTML file will be saved.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(html_object, requests.models.Response):\n",
    "        html_text = html_object.text\n",
    "    elif isinstance(html_object, (bs4.BeautifulSoup,bs4.element.Tag)):\n",
    "        html_text = str(html_object.prettify())\n",
    "    try:\n",
    "        with open(path,'w') as f:\n",
    "            f.write(html_text)\n",
    "    except Exception as err:\n",
    "        print('[Error] The response object you provided cannot be turned into beautiful soup object: '+str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_readable_content(content):\n",
    "    \"\"\"Return whether the content passed is a readable content like Tag or NavigableString; not CData, Comment, Declaration, Doctype, ProcessingInstruction, ResultSet, Script, Stylesheet, XMLFormatter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    # Types that are instances of NavigableString:  CData, Comment, Declaration, Doctype, PreformattedString, ProcessingInstruction, ResultSet, Script, Stylesheet, TemplateString, XMLFormatter\n",
    "    # Types in the group above that are not String:  CData, Comment, Declaration, Doctype, ProcessingInstruction, ResultSet, Script, Stylesheet, XMLFormatter\n",
    "    return isinstance(content, (bs4.element.Tag, bs4.element.NavigableString)) and not isinstance(content, (bs4.element.CData, bs4.element.Comment, bs4.element.Declaration, bs4.element.Doctype, bs4.element.ProcessingInstruction, bs4.element.ResultSet, bs4.element.Script, bs4.element.Stylesheet, bs4.element.XMLFormatter))\n",
    "\n",
    "def get_contents(element):\n",
    "    \"\"\"Return a list of non-empty and readable contents/children of the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of bs4.element\n",
    "        \n",
    "    \"\"\"\n",
    "    return [content for content in element.contents if str(content).strip()!='' and is_readable_content(content)]\n",
    "\n",
    "def get_contents_names(element):\n",
    "    \"\"\"Return the list of names of the non-empty and readable contents/children of the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    return [content.name for content in get_contents(element)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elevate_to_nearest_tag(element):\n",
    "    if isinstance(element, bs4.element.NavigableString):\n",
    "        element = element.parent\n",
    "    if isinstance(element, bs4.element.Tag):\n",
    "        return element\n",
    "    else:\n",
    "        print('[Error] Element is still not Tag after getting the parent.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dispatch(str)\n",
    "def highlight_element(element, highlight_style = \"background-color: rgba(255,0,0,0.5); border: 3px dotted yellow\"):\n",
    "    element['style'] = highlight_style\n",
    "\n",
    "@dispatch(list)\n",
    "def highlight_element(elements, highlight_style = \"background-color: rgba(255,0,0,0.5); border: 3px dotted yellow; \"):\n",
    "    for element in elements:\n",
    "        element['style'] = highlight_style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_like(string):   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get_soup(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Brooklyn: 3rd Avenue - St. Mark's Place\"\n",
    "\n",
    "# 'RECENTLY DIGITIZED ITEMS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def give_unique_sample_element(soup):\n",
    "    attempt_count = 0\n",
    "    matched_elements = []\n",
    "    while len(matched_elements)!=1:\n",
    "        if attempt_count>0:\n",
    "            print('\\nThere are '+str(len(matched_elements))+' matched elements given your last input. They are:\\n'+'\\t\\n'.join([str(matched_element)[:100] for matched_element in matched_elements])+'\\n\\n')\n",
    "        displayed_text = input('What is the displayed text for one of the elements you want to scrape:\\n')\n",
    "        matched_elements = soup.find_all(text = re.compile(displayed_text,re.IGNORECASE))\n",
    "        attempt_count += 1\n",
    "    sample_element = matched_elements[0]\n",
    "    sample_element = elevate_to_nearest_tag(sample_element)\n",
    "    print('\\nUnique match is found:\\n'+str(sample_element)+'\\n\\n')\n",
    "    return sample_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_self_index(element):\n",
    "    self_type = element.name\n",
    "    previous_siblings_of_all_types = list(element.previous_siblings)\n",
    "    previous_siblings_of_same_type = [element for element in previous_siblings_of_all_types if element.name == self_type]\n",
    "    return len(previous_siblings_of_same_type) + 1 # css selector starts indexing with 1 instead of 0\n",
    "\n",
    "# Reference: https://stackoverflow.com/a/32263260 (basic structure inspiration)\n",
    "# Reference: https://csswizardry.com/2012/05/keep-your-css-selectors-short (tips to improve efficiency)\n",
    "\n",
    "def describe_element_in_css(node):\n",
    "    \n",
    "    enough_to_be_unique = False\n",
    "    \n",
    "    node_type = node.name\n",
    "    \n",
    "    node_attrs = node.attrs\n",
    "    node_attrs_string = ''\n",
    "    for k,v in node_attrs.items():\n",
    "        if k == 'id':\n",
    "            node_attrs_string += '#' + node_attrs[k]\n",
    "            enough_to_be_unique = True\n",
    "            break\n",
    "        elif k == 'class':\n",
    "            node_attrs_string += '.'+'.'.join(node_attrs[k])\n",
    "\n",
    "    element_part = node_type + node_attrs_string\n",
    "            \n",
    "    if not enough_to_be_unique:\n",
    "        length = get_self_index(node)\n",
    "        if (length) > 1:\n",
    "            element_part = '%s:nth-child(%s)' % (element_part, length)\n",
    "        \n",
    "    return element_part\n",
    "\n",
    "def get_css_path(node):\n",
    "    path = [describe_element_in_css(node)]\n",
    "    for parent in node.parents:\n",
    "        if parent.name == 'body' or '#' in path[0]:\n",
    "            break\n",
    "        path.insert(0, describe_element_in_css(parent))\n",
    "    return ' > '.join(path)\n",
    "\n",
    "@dispatch(str)\n",
    "def extract_text(element):\n",
    "    return element.text.strip()\n",
    "\n",
    "@dispatch(list)\n",
    "def extract_text(elements):\n",
    "    return [extract_text(element) for element in elements]\n",
    "\n",
    "def elevate_css_path(path):\n",
    "    return '>'.join(path.split('>')[:-1]).strip() if '>' in path else path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the displayed text for one of the elements you want to scrape:\n",
      "collection of photographs \n",
      "\n",
      "There are 6 matched elements given your last input. They are:\n",
      "\n",
      "  var bg_images = [{\"title\":\"Penn Station, Interior, Manhattan.\",\"name\":\"482603.jpg\",\"collection_id\t\n",
      "Collection of photographs of New York City\t\n",
      "Collection of photographs of New York City, New York State and more by Max Hubacher\t\n",
      "Collection of photographs of New York City, 1931-1942\t\n",
      "Collection of photographs of East River and Hudson River piers, Manhattan\t\n",
      "Collection of photographs of New York City\n",
      "\n",
      "\n",
      "What is the displayed text for one of the elements you want to scrape:\n",
      "COLLECTION OF PHOTOGRAPHS OF NEW YORK CITY, NEW YORK\n",
      "\n",
      "Unique match is found:\n",
      "<h5>Collection of photographs of New York City, New York State and more by Max Hubacher</h5>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# André Fashion Illustrations\n",
    "sample_element = give_unique_sample_element(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = get_css_path(sample_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = elevate_css_path(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlighted_soup = highlight_element(soup.select(path))\n",
    "save_html(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "\n",
    "References:   \n",
    " - https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    " - https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: file saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filetype = url.split('.')[-1]\n",
    "# filetype = 'html'\n",
    "# filename = re.sub(r'^https?://','',url).replace('.','-').replace('/','_') + '.' + filetype\n",
    "\n",
    "# with open(filename,'w') as f:\n",
    "#     f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for To-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# # Reference: How to fake and rotate User Agents using Python 3\n",
    "# # https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "# import requests\n",
    "# import random \n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # This data was created by using the curl method explained above\n",
    "# headers_list = [\n",
    "#     # Firefox 77 Mac\n",
    "#      {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\"\n",
    "#     },\n",
    "#     # Firefox 77 Windows\n",
    "#     {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\"\n",
    "#     },\n",
    "#     # Chrome 83 Mac\n",
    "#     {\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "#         \"Sec-Fetch-Site\": \"none\",\n",
    "#         \"Sec-Fetch-Mode\": \"navigate\",\n",
    "#         \"Sec-Fetch-Dest\": \"document\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "#     },\n",
    "#     # Chrome 83 Windows \n",
    "#     {\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "#         \"Sec-Fetch-Site\": \"same-origin\",\n",
    "#         \"Sec-Fetch-Mode\": \"navigate\",\n",
    "#         \"Sec-Fetch-User\": \"?1\",\n",
    "#         \"Sec-Fetch-Dest\": \"document\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "#     }\n",
    "# ]\n",
    "# # Create ordered dict from Headers above\n",
    "# ordered_headers_list = []\n",
    "# for headers in headers_list:\n",
    "#     h = OrderedDict()\n",
    "#     for header,value in headers.items():\n",
    "#         h[header]=value\n",
    "#     ordered_headers_list.append(h)\n",
    "    \n",
    "    \n",
    "# url = 'https://httpbin.org/headers'\n",
    "\n",
    "# for i in range(1,4):\n",
    "#     #Pick a random browser headers\n",
    "#     headers = random.choice(headers_list)\n",
    "#     #Create a request session\n",
    "#     r = requests.Session()\n",
    "#     r.headers = headers\n",
    "    \n",
    "#     response = r.get(url)\n",
    "#     print(\"Request #%d\\nUser-Agent Sent:%s\\n\\nHeaders Recevied by HTTPBin:\"%(i,headers['User-Agent']))\n",
    "#     print(response.json())\n",
    "#     print(\"-------------------\")\n",
    "\n",
    "# ################################################################################\n",
    "# # Reference: How To Rotate Proxies and change IP Addresses using Python 3\n",
    "# # https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/    \n",
    "    \n",
    "# from lxml.html import fromstring\n",
    "# import requests\n",
    "# from itertools import cycle\n",
    "# import traceback\n",
    "\n",
    "# def get_proxies():\n",
    "#     url = 'https://free-proxy-list.net/'\n",
    "#     response = requests.get(url)\n",
    "#     parser = fromstring(response.text)\n",
    "#     proxies = set()\n",
    "#     for i in parser.xpath('//tbody/tr')[:10]:\n",
    "#         if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "#             proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "#             proxies.add(proxy)\n",
    "#     return proxies\n",
    "\n",
    "\n",
    "# #If you are copy pasting proxy ips, put in the list below\n",
    "# #proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "# proxies = get_proxies()\n",
    "# proxy_pool = cycle(proxies)\n",
    "\n",
    "# url = 'https://httpbin.org/ip'\n",
    "# for i in range(1,11):\n",
    "#     #Get a proxy from the pool\n",
    "#     proxy = next(proxy_pool)\n",
    "#     print(\"Request #%d\"%i)\n",
    "#     try:\n",
    "#         response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "#         print(response.json())\n",
    "#     except:\n",
    "#         #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "#         #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "#         print(\"Skipping. Connnection error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
