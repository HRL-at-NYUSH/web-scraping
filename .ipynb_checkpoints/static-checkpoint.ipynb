{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This is a develop environment for the static website scraper. Functions developed here will be eventually packaged into .py files and called from other notebook or python scripts.\n",
    "\n",
    "\n",
    "**Progress**\n",
    "\n",
    "Done:\n",
    "    1. get response\n",
    "    2. find and extract content\n",
    "    3. save file\n",
    "\n",
    "To-do:\n",
    "    4. pagination\n",
    "    \n",
    "Long-term:\n",
    "    - handle special encoding\n",
    "    - rotate agent\n",
    "    - rotate IP\n",
    "    - support authentication \n",
    "    - support cookie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure you install the required libraries\n",
    "\n",
    "# !pip3 install --upgrade requests # library for making request for the static websites\n",
    "# !pip3 install --upgrade soupsieve  # library to support css selector in beautifulsoup\n",
    "# !pip3 install --upgrade beautifulsoup4 # a parser that balances between efficiency and leniency\n",
    "# !pip3 install --upgrade --user lxml # a more efficient parser\n",
    "# !pip3 install --upgrade html5lib # a parser that acts like a browser, most lenient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key libraries\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "import requests\n",
    "import bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions help us understand the variables that exist in the environment\n",
    "# which is useful for creating natural language interface for data analysis\n",
    "\n",
    "def get_local_variables(ignore_underscore = True):\n",
    "    \"\"\"Get the name and definition of the local variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dictionary\n",
    "        A mapping between name and definition of the local variables.\n",
    "                \n",
    "    \"\"\"\n",
    "    callers_local_vars = dict(inspect.currentframe().f_back.f_locals.items())\n",
    "    if filter_:\n",
    "        var_keys = list(callers_local_vars.keys())\n",
    "        for key in var_keys:\n",
    "            if key.startswith('_'):\n",
    "                del callers_local_vars[key]\n",
    "    return callers_local_vars\n",
    "def retrieve_name(var):\n",
    "    \"\"\"Retrieve the name of the variable. # Reference https://stackoverflow.com/a/40536047.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var: object \n",
    "        Variable to get the name of.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        Name of the variable passed.\n",
    "        \n",
    "    \"\"\"\n",
    "    for fi in reversed(inspect.stack()):\n",
    "        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "        if len(names) > 0:\n",
    "            return names[0]\n",
    "        \n",
    "def get_attributes(obj, ignore_underscore = True):\n",
    "    \"\"\"Get a list of valid attributes of the object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A list of valid attributes of the object.\n",
    "                \n",
    "    \"\"\"\n",
    "    return [x for x in dir(obj) if not x.startswith('_')]\n",
    "\n",
    "def print_attributes_and_values(obj, ignore_underscore = True):\n",
    "    \"\"\"Print the valid attributes of the object and their corresponding values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "                \n",
    "    \"\"\"\n",
    "    obj_name = retrieve_name(obj)\n",
    "    attributes = get_attributes(obj, ignore_underscore = ignore_underscore)\n",
    "    for attr in attributes:\n",
    "        obj_attr_string = obj_name+'.'+attr\n",
    "        print(obj_attr_string)\n",
    "        print(' '*4 + str(eval(obj_attr_string))[:60])\n",
    "        print('-'*70)\n",
    "\n",
    "\n",
    "def get_response(url, verbose = True):\n",
    "    \"\"\"Get the response of the HTTP GET request for the target url.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        The url to the website that needs to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    response object\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise Exception when response was not successful\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print('[Error] HTTP error occurred: '+str(http_err))\n",
    "        return requests.models.Response() # Return empty response\n",
    "    except Exception as err:\n",
    "        print('[Error] Other error occurred: '+str(err))\n",
    "        return requests.models.Response() # Return empty response\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('[Success] The website at \"'+url+'\" is collected succesfully.')\n",
    "        return response\n",
    "\n",
    "def get_responses(urls, verbose = True):\n",
    "    \"\"\"Get the responses of the HTTP GET requests for the target urls. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    urls: list of string\n",
    "        The urls to the websites that need to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of response object\n",
    "        \n",
    "    \"\"\"\n",
    "    return [get_response(url) for url in urls]\n",
    "\n",
    "\n",
    "\n",
    "def get_soup(response, default_parser = 'lxml'):\n",
    "    \"\"\"Get the beautiful soup object of the response object or filepath or html string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.models.Response, string\n",
    "        The response object or filepath or html string. \n",
    "    default_parser: string (optional, default = lxml)\n",
    "        Which parser to use when parsing the response.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of response object\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(response, requests.models.Response):\n",
    "        soup = bs4.BeautifulSoup(response.content, default_parser)\n",
    "    elif isinstance(response, str) and os.path.exists(response):\n",
    "        with open(response) as file_handler:\n",
    "            soup = bs4.BeautifulSoup(file_handler, default_parser)\n",
    "    else:\n",
    "        try:\n",
    "            soup = bs4.BeautifulSoup(response, default_parser)\n",
    "        except Exception as err:\n",
    "            print('[Error] The response object you provided cannot be turned into beautiful soup object: '+str(err))\n",
    "    return soup\n",
    "\n",
    "def save_html(html_object, url , path = ''):\n",
    "    \"\"\"Save the response or soup object as a HTML file at the path provided.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    html_object: requests.models.Response, bs4.BeautifulSoup\n",
    "        The response or soup object. \n",
    "    path: string (optional, default = ./TEMP.html)\n",
    "        The path at which the HTML file will be saved.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "        \n",
    "    \"\"\"\n",
    "    if path == '':\n",
    "        path = './'+re.sub('^https?://','',url).replace('/','_').replace('.','-')+'.html'\n",
    "    if isinstance(html_object, requests.models.Response):\n",
    "        html_text = html_object.text\n",
    "    elif isinstance(html_object, (bs4.BeautifulSoup,bs4.element.Tag)):\n",
    "        html_text = str(html_object.prettify())\n",
    "    else:\n",
    "        html_text = str(html_object)\n",
    "    try:\n",
    "        with open(path,'w') as f:\n",
    "            f.write(html_text)\n",
    "            print('[Success] The HTML file is saved succesfully.')\n",
    "    except Exception as err:\n",
    "        print('[Error] The response object you provided cannot be turned into beautiful soup object: '+str(err))\n",
    "\n",
    "def is_readable_content(content):\n",
    "    \"\"\"Return whether the content passed is a readable content like Tag or NavigableString; not CData, Comment, Declaration, Doctype, ProcessingInstruction, ResultSet, Script, Stylesheet, XMLFormatter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    # Types that are instances of NavigableString:  CData, Comment, Declaration, Doctype, PreformattedString, ProcessingInstruction, ResultSet, Script, Stylesheet, TemplateString, XMLFormatter\n",
    "    # Types in the group above that are not String:  CData, Comment, Declaration, Doctype, ProcessingInstruction, ResultSet, Script, Stylesheet, XMLFormatter\n",
    "    return isinstance(content, (bs4.element.Tag, bs4.element.NavigableString)) and not isinstance(content, (bs4.element.CData, bs4.element.Comment, bs4.element.Declaration, bs4.element.Doctype, bs4.element.ProcessingInstruction, bs4.element.ResultSet, bs4.element.Script, bs4.element.Stylesheet, bs4.element.XMLFormatter))\n",
    "\n",
    "def get_contents(element):\n",
    "    \"\"\"Return a list of non-empty and readable contents/children of the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of bs4.element\n",
    "        \n",
    "    \"\"\"\n",
    "    return [content for content in element.contents if str(content).strip()!='' and is_readable_content(content)]\n",
    "\n",
    "def get_contents_names(element):\n",
    "    \"\"\"Return the list of names of the non-empty and readable contents/children of the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    return [content.name for content in get_contents(element)]\n",
    "\n",
    "def elevate_till_is_tag(element):\n",
    "    \"\"\"Return the nearest Tag element, if not itself, return its parent if it is a Tag element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    bs4.element.Tag\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(element, bs4.element.NavigableString):\n",
    "        return element.parent\n",
    "    if isinstance(element, bs4.element.Tag):\n",
    "        return element\n",
    "    else:\n",
    "        print('[Error] Element is still not Tag after getting the parent.')\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_self_index(element):\n",
    "    \"\"\"Return the index of the element among its siblings.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    int\n",
    "        \n",
    "    \"\"\"\n",
    "    self_type = element.name\n",
    "    previous_siblings_of_all_types = list(element.previous_siblings)\n",
    "    previous_siblings_of_same_type = [element for element in previous_siblings_of_all_types if element.name == self_type]\n",
    "    return len(previous_siblings_of_same_type) + 1 # css selector starts indexing with 1 instead of 0\n",
    "\n",
    "\n",
    "# Reference: https://stackoverflow.com/a/32263260 (basic structure inspiration)\n",
    "# Reference: https://csswizardry.com/2012/05/keep-your-css-selectors-short (tips to improve efficiency)\n",
    "\n",
    "def describe_part_of_css_selector(node):\n",
    "    \"\"\"Construct part of the css selector path.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    enough_to_be_unique = False\n",
    "    \n",
    "    node_type = node.name\n",
    "    \n",
    "    node_attrs = node.attrs\n",
    "    node_attrs_string = ''\n",
    "    for k,v in node_attrs.items():\n",
    "        if k == 'id':\n",
    "            node_attrs_string += '#' + node_attrs[k]\n",
    "            enough_to_be_unique = True\n",
    "            break\n",
    "        elif k == 'class':\n",
    "            node_attrs_string += '.'+'.'.join(node_attrs[k])\n",
    "\n",
    "    element_part = node_type + node_attrs_string\n",
    "            \n",
    "    if not enough_to_be_unique:\n",
    "        length = get_self_index(node)\n",
    "        if (length) > 1:\n",
    "            element_part = '%s:nth-child(%s)' % (element_part, length)\n",
    "        \n",
    "    return element_part\n",
    "\n",
    "def get_css_selector_path(node):\n",
    "    \"\"\"Construct the whole css selector path to a certain element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    path = [describe_part_of_css_selector(node)]\n",
    "    for parent in node.parents:\n",
    "        if parent.name == 'body' or '#' in path[0]:\n",
    "            break\n",
    "        path.insert(0, describe_part_of_css_selector(parent))\n",
    "    return ' > '.join(path)\n",
    "\n",
    "def elevate_css_selector_path(path):\n",
    "    \"\"\"Get the css selector path to the element that is one level above the current element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        The css selector path to an BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return '>'.join(path.split('>')[:-1]).strip() if '>' in path else path\n",
    "\n",
    "\n",
    "from collections.abc import Iterable\n",
    "def is_iterable(obj):\n",
    "    \"\"\"Check if the passed object is iterable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obj: object\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return isinstance(obj, Iterable)\n",
    "\n",
    "\n",
    "def flatten_list(l):\n",
    "    \"\"\"Flatten a list of lists to a one-layer list (elements are in original order). Note this is NOT recursive, meaning multi-layered list of lists cannot be converted into a single-layered list in one transformation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l: list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def extract_text(element):\n",
    "    \"\"\"Extract the textual content of an element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return element.text.strip()\n",
    "\n",
    "\n",
    "def get_directly_related_link(element):\n",
    "    \"\"\"Extract the link directly related to the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    while element.name != 'a' and count < 5:\n",
    "        element = element.parent\n",
    "        count += 1\n",
    "    if element.name != 'a':\n",
    "        return ''\n",
    "    else:\n",
    "        return element['href']\n",
    "\n",
    "\n",
    "def get_indirectly_related_links(element):\n",
    "    \"\"\"Extract the links indirectly related to the element (i.e. belonging to the sibling elements).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return [link['href'] for link in element.parent.find_all('a')]\n",
    "\n",
    "\n",
    "def get_related_link(element):\n",
    "    \"\"\"Extract the link directly related to the element, if none is found, get indirectly related links.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string or list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    link = get_directly_related_link(element)\n",
    "    \n",
    "    if link != '':\n",
    "        return link\n",
    "    else:\n",
    "        links = get_indirectly_related_links(element)\n",
    "        if len(links) == 1:\n",
    "            return links[0]\n",
    "        else:\n",
    "            return links\n",
    "\n",
    "def get_longest_separator(text):\n",
    "    \"\"\"Return the longest separator (formed by multiple newline) in the text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: string\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(text, str) and '\\n' in text:\n",
    "        return max(re.findall(r'\\n+', text, re.DOTALL), key=lambda x: len(x))\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def get_longest_separator_in_list(texts):\n",
    "    \"\"\"Return the longest separator (formed by multiple newline) in the texts contained in the list.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    texts: list of string\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    return max([get_longest_separator(text) for text in texts], key=len)\n",
    "\n",
    "\n",
    "def remove_blank_element_in_list(li):\n",
    "    \"\"\"Return a cleaned version of the list with all blank elements removed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    li: list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        \n",
    "    \"\"\"\n",
    "    return [element for element in li if element.strip()!='']\n",
    "\n",
    "def recursive_split(text):\n",
    "    \"\"\"Return a multi-layer list of lists resulting from a recursive split of the text (split by longer separator first).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: String\n",
    "        A piece of text that contains separators of different lengths.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list (of lists)\n",
    "        \n",
    "    \"\"\"\n",
    "    longest_separator = get_longest_separator(text)\n",
    "    if longest_separator == '':\n",
    "        return text\n",
    "    else:\n",
    "        return [recursive_split(part) for part in remove_blank_element_in_list(text.split(longest_separator))]\n",
    "    \n",
    "def get_unique_sample_element(soup, target_phrase = '', context_radius = 40):\n",
    "    \"\"\"Find and return an element based on the html structure and a target phrase, solicit additional information from user through input questions if needed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup: bs4.soup\n",
    "        The parsed tree of the response.\n",
    "    target_phrase: string (optional, if not provided, the function will ask user to input)\n",
    "        The phrase used to find the sample element.\n",
    "    context_radius: int (optional, default = 40)\n",
    "        How many characters to display to help user choose recurring phrases based on their contexts.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    bs4.element.Tag\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    attempt_count = 0\n",
    "    matched_elements = []\n",
    "    \n",
    "    if target_phrase != '':\n",
    "        target_phrase = target_phrase.lower()\n",
    "        matched_elements = soup.find_all(text = re.compile(target_phrase,re.IGNORECASE))\n",
    "        attempt_count += 1\n",
    "    \n",
    "    while len(matched_elements)!=1:\n",
    "    \n",
    "        if attempt_count>0:\n",
    "            print('\\nThere are '+str(len(matched_elements))+' matched elements given your last input. They are:\\n'+'\\t\\n'.join([str(matched_element)[:100] for matched_element in matched_elements[:10]])+'\\n\\n')\n",
    "        \n",
    "        if len(set([str(matched_element) for matched_element in matched_elements]))==1:\n",
    "            last_index = -1\n",
    "            phrases_in_context = []\n",
    "            whole_page_text = re.sub('\\s+',' ',soup.text).lower()\n",
    "            \n",
    "            if whole_page_text.count(target_phrase) == len(matched_elements):\n",
    "            \n",
    "                for i in range(whole_page_text.count(target_phrase)):\n",
    "                    current_index = whole_page_text.index(target_phrase,last_index+1)\n",
    "                    phrases_in_context.append(whole_page_text[current_index-context_radius:current_index]+'\\\\\\\\ '+whole_page_text[current_index:current_index+len(target_phrase)]+' //'+whole_page_text[current_index+len(target_phrase):current_index+len(target_phrase)+context_radius])\n",
    "                    last_index = current_index\n",
    "                if len(set(phrases_in_context))==1:\n",
    "                    print('[Error] There are '+str(len(phrases_in_context))+' occurences of the same target phrase on the page that have very similar contexts.\\nPlease use the browser inspector tool to copy the \"selector\" or \"Selector Path\".\\n')\n",
    "                    return None\n",
    "                else:\n",
    "                    numbered_contexts = ''\n",
    "                    for i in range(len(phrases_in_context)):\n",
    "                        numbered_contexts += 'Choice '+str(i+1)+':  '+phrases_in_context[i] + '\\n'\n",
    "                    print('There are '+str(len(phrases_in_context))+' occurences of the same target phrase on the page,\\nplease choose one based on their contexts:\\n\\n' + numbered_contexts + '\\n')\n",
    "\n",
    "                which_one = 0\n",
    "                while which_one-1 not in range(len(phrases_in_context)):\n",
    "                    which_one = input('Which choice is the element you that want to scrape: [1, 2, 3, ...]\\n')\n",
    "                    try:\n",
    "                        which_one = int(which_one)\n",
    "                    except:\n",
    "                        which_one = 0\n",
    "                matched_elements = [matched_elements[which_one-1]]\n",
    "                \n",
    "            else:\n",
    "                print('[Error] The number of matched elements and the number of target phrase occurences are not the same.\\nPlease use the browser inspector tool to copy the \"selector\" or \"Selector Path\".\\n')\n",
    "                return None\n",
    "            \n",
    "        else:\n",
    "            target_phrase = input('What is the displayed text for one of the elements you want to scrape: '+('(Type \"QUIT\" to stop)' if attempt_count>3 else '')+'\\n')\n",
    "            if target_phrase == 'QUIT':\n",
    "                print('\\n[Error] It is very likely that the website is not fully collected.\\n        Please try this command: scrape_where_for_what(YOUR_URL, YOUR_TARGET_PHRASE, save_as_html = True)\\n        A HTML file will be created in your local folder, open it with a browser.\\n        If you cannot see what you want to find on the page, please switch to dynamic scraping method.\\n')\n",
    "                return None\n",
    "            matched_elements = soup.find_all(text = re.compile(target_phrase,re.IGNORECASE))\n",
    "            attempt_count += 1\n",
    "            \n",
    "    sample_element = matched_elements[0]\n",
    "    sample_element = elevate_till_is_tag(sample_element)\n",
    "    print('\\nUnique match is found:\\n'+str(sample_element)[:100]+ (' ......' if len(str(sample_element))>100 else '') +'\\n\\n')\n",
    "    \n",
    "    if sample_element.name == 'script':\n",
    "        matched_lines = [line for line in sample_element.prettify().split('\\n') if target_phrase in line.lower()]\n",
    "        try:\n",
    "            assert(len(matched_lines)==1)\n",
    "            matched_line = matched_lines[0].strip().strip(';')\n",
    "            matched_data = matched_line.split('=',maxsplit=1)[1].strip()\n",
    "            data = pd.DataFrame(json.loads(matched_data))\n",
    "            return data\n",
    "        except:\n",
    "            print('[Error] There are multiple occurences of the target phrase in the JS script.\\nPlease use another more unique target phrase or inspect the page source for the data in JS script.\\n')\n",
    "            return None\n",
    "    \n",
    "    return sample_element\n",
    "\n",
    "def extract_contents(soup, path):\n",
    "    \"\"\"Extract and return the texts and links with the target path in the parsed tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup: bs4.soup\n",
    "        The parsed tree of the response.\n",
    "    path: string\n",
    "        The css selector path to the target elements.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pd.DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(soup, pd.DataFrame):\n",
    "        return soup\n",
    "    \n",
    "    target_elements = soup.select(path)\n",
    "\n",
    "    extracted_contents = pd.DataFrame([(recursive_split(extract_text(target_element)), get_related_link(target_element)) for target_element in target_elements], columns = ['text','url'])\n",
    "\n",
    "    return extracted_contents\n",
    "\n",
    "def scrape_where_for_what(url, target_phrase, go_up = 0, save_as_html = False):\n",
    "    \"\"\"Get the parsed tree of the website and the path to the target elements.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        The url of the website you want to scrape.\n",
    "    target_phrase: string\n",
    "        The displayed text of one of the elements you want to scrape.\n",
    "    go_up: int\n",
    "        How many levels to go up in order to get the amount of contents you want.\n",
    "    save_as_html: boolean (optional, default = False)\n",
    "        Whether or not to save the response from the website as a HTML file.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pd.DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_response(url)\n",
    "    \n",
    "    if save_as_html:\n",
    "        save_html(response.text, url)\n",
    "    \n",
    "    soup = get_soup(response)\n",
    "\n",
    "    sample_element = get_unique_sample_element(soup, target_phrase)\n",
    "    \n",
    "    if sample_element is None:\n",
    "        return None, None\n",
    "    \n",
    "    if isinstance(sample_element, pd.DataFrame):\n",
    "        print('[Success] Data is in the JS script and now extracted as a DataFrame into the variable \"soup\".\\n')\n",
    "        return sample_element\n",
    "    \n",
    "    sample_path = get_css_selector_path(sample_element)\n",
    "    \n",
    "    path = sample_path[:]\n",
    "    for i in range(go_up):\n",
    "        path = elevate_css_selector_path(path)\n",
    "    \n",
    "    extracted_contents = extract_contents(soup, path)\n",
    "\n",
    "    return extracted_contents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"https://digitalcollections.nypl.org\" is collected succesfully.\n",
      "\n",
      "Unique match is found:\n",
      "<h5>The Black Experience in Children's Books: Selections from Augusta Baker's Bibliographies</h5>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Black Experience in Children's Books: Selections from Augusta Baker's Bibliographies</td>\n",
       "      <td>/collections/the-black-experience-in-childrens-books-selections-from-augusta-bakers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scrapbooks of New York City views</td>\n",
       "      <td>/collections/scrapbooks-of-new-york-city-views</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Li ji ji shi: er shi wu juan</td>\n",
       "      <td>/collections/li-ji-ji-shi-er-shi-wu-juan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Women of distinction: remarkable in works and invincible in character</td>\n",
       "      <td>/collections/women-of-distinction-remarkable-in-works-and-invincible-in-character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Collection of ledgers and cash books covering the period 1891-1925</td>\n",
       "      <td>/collections/collection-of-ledgers-and-cash-books-covering-the-period-1891-1925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>William Blake: Illuminated Books</td>\n",
       "      <td>/collections/william-blake-illuminated-books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>Ise Monogatari Emaki</td>\n",
       "      <td>/collections/ise-monogatari-emaki</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>Des cleres et nobles femmes</td>\n",
       "      <td>/collections/des-cleres-et-nobles-femmes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>Minchô shiken (The Colored Inkstone of the Ming Period)</td>\n",
       "      <td>/collections/minch-shiken-the-colored-inkstone-of-the-ming-period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>Tsuki hyakushi</td>\n",
       "      <td>/collections/tsuki-hyakushi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>230 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                         text  \\\n",
       "0    The Black Experience in Children's Books: Selections from Augusta Baker's Bibliographies   \n",
       "1                                                           Scrapbooks of New York City views   \n",
       "2                                                                Li ji ji shi: er shi wu juan   \n",
       "3                       Women of distinction: remarkable in works and invincible in character   \n",
       "4                          Collection of ledgers and cash books covering the period 1891-1925   \n",
       "..                                                                                        ...   \n",
       "225                                                          William Blake: Illuminated Books   \n",
       "226                                                                      Ise Monogatari Emaki   \n",
       "227                                                               Des cleres et nobles femmes   \n",
       "228                                   Minchô shiken (The Colored Inkstone of the Ming Period)   \n",
       "229                                                                            Tsuki hyakushi   \n",
       "\n",
       "                                                                                     url  \n",
       "0    /collections/the-black-experience-in-childrens-books-selections-from-augusta-bakers  \n",
       "1                                         /collections/scrapbooks-of-new-york-city-views  \n",
       "2                                               /collections/li-ji-ji-shi-er-shi-wu-juan  \n",
       "3      /collections/women-of-distinction-remarkable-in-works-and-invincible-in-character  \n",
       "4        /collections/collection-of-ledgers-and-cash-books-covering-the-period-1891-1925  \n",
       "..                                                                                   ...  \n",
       "225                                         /collections/william-blake-illuminated-books  \n",
       "226                                                    /collections/ise-monogatari-emaki  \n",
       "227                                             /collections/des-cleres-et-nobles-femmes  \n",
       "228                    /collections/minch-shiken-the-colored-inkstone-of-the-ming-period  \n",
       "229                                                          /collections/tsuki-hyakushi  \n",
       "\n",
       "[230 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_where_for_what('https://digitalcollections.nypl.org', \"Black Experience in Children's Book\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"https://digitalcollections.nypl.org/collections/changing-new-york\" is collected succesfully.\n",
      "\n",
      "Unique match is found:\n",
      "<script type=\"text/javascript\">\n",
      "\n",
      "  var search_results = [{\"restricted\":false,\"item\":{\"id\":\"510d47d9- ......\n",
      "\n",
      "\n",
      "[Success] Data is in the JS script and now extracted as a DataFrame into the variable \"soup\".\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restricted</th>\n",
       "      <th>item</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f9d-a3d9-e040-e00a18064a99', 'title': 'Rope store, South Street and James Slip, Manhattan.', 'image_id': '482824', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/T2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f4a-a3d9-e040-e00a18064a99', 'title': 'Automat, 977 Eighth Avenue, Manhattan.', 'image_id': '482752', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/sAeaJhzFT5-wk6k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f85-a3d9-e040-e00a18064a99', 'title': 'Columbus Circle, Manhattan.', 'image_id': '482580', 'multi': True, 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/6b_qtKEqTYC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f00-a3d9-e040-e00a18064a99', 'title': 'Broadway and Thomas Street, Manhattan.', 'image_id': '482689', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/BBYhX4MbQXSp1S9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f13-a3d9-e040-e00a18064a99', 'title': 'Broadway and Thomas Street, Manhattan.', 'image_id': '482706', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/fgkkXRSHSruvd-Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4eb0-a3d9-e040-e00a18064a99', 'title': 'Columbia Presbyterian Medical Center, 168th Street and Broadway, from 165th Street and Riverside Drive, Manhattan.', 'image_id': '482622', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f02-a3d9-e040-e00a18064a99', 'title': 'Gay Street no. 14-16, Manhattan.', 'image_id': '482690', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/0-YF-9etQLOSL4VBoVjFJ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47df-335a-a3d9-e040-e00a18064a99', 'title': 'Riverside Drive, no. 857, at 159th Street, Manhattan.', 'image_id': '1219146', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f6f-a3d9-e040-e00a18064a99', 'title': 'George Washington Bridge, Riverside Drive and 179th Street, Manhattan.', 'image_id': '482785', 'sequence_number': 1, 'high_res_link': 'http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>False</td>\n",
       "      <td>{'id': '510d47d9-4f20-a3d9-e040-e00a18064a99', 'title': 'Salmagundi Club, 47 Fifth Avenue, Manhattan', 'image_id': '482716', 'multi': True, 'sequence_number': 1, 'high_res_link': 'http://link.nypl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     restricted  \\\n",
       "0         False   \n",
       "1         False   \n",
       "2         False   \n",
       "3         False   \n",
       "4         False   \n",
       "..          ...   \n",
       "195       False   \n",
       "196       False   \n",
       "197       False   \n",
       "198       False   \n",
       "199       False   \n",
       "\n",
       "                                                                                                                                                                                                        item  \n",
       "0    {'id': '510d47d9-4f9d-a3d9-e040-e00a18064a99', 'title': 'Rope store, South Street and James Slip, Manhattan.', 'image_id': '482824', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/T2...  \n",
       "1    {'id': '510d47d9-4f4a-a3d9-e040-e00a18064a99', 'title': 'Automat, 977 Eighth Avenue, Manhattan.', 'image_id': '482752', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/sAeaJhzFT5-wk6k...  \n",
       "2    {'id': '510d47d9-4f85-a3d9-e040-e00a18064a99', 'title': 'Columbus Circle, Manhattan.', 'image_id': '482580', 'multi': True, 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/6b_qtKEqTYC...  \n",
       "3    {'id': '510d47d9-4f00-a3d9-e040-e00a18064a99', 'title': 'Broadway and Thomas Street, Manhattan.', 'image_id': '482689', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/BBYhX4MbQXSp1S9...  \n",
       "4    {'id': '510d47d9-4f13-a3d9-e040-e00a18064a99', 'title': 'Broadway and Thomas Street, Manhattan.', 'image_id': '482706', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/fgkkXRSHSruvd-Y...  \n",
       "..                                                                                                                                                                                                       ...  \n",
       "195  {'id': '510d47d9-4eb0-a3d9-e040-e00a18064a99', 'title': 'Columbia Presbyterian Medical Center, 168th Street and Broadway, from 165th Street and Riverside Drive, Manhattan.', 'image_id': '482622', ...  \n",
       "196  {'id': '510d47d9-4f02-a3d9-e040-e00a18064a99', 'title': 'Gay Street no. 14-16, Manhattan.', 'image_id': '482690', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org/0-YF-9etQLOSL4VBoVjFJ...  \n",
       "197  {'id': '510d47df-335a-a3d9-e040-e00a18064a99', 'title': 'Riverside Drive, no. 857, at 159th Street, Manhattan.', 'image_id': '1219146', 'sequence_number': 1, 'high_res_link': 'http://link.nypl.org...  \n",
       "198  {'id': '510d47d9-4f6f-a3d9-e040-e00a18064a99', 'title': 'George Washington Bridge, Riverside Drive and 179th Street, Manhattan.', 'image_id': '482785', 'sequence_number': 1, 'high_res_link': 'http...  \n",
       "199  {'id': '510d47d9-4f20-a3d9-e040-e00a18064a99', 'title': 'Salmagundi Club, 47 Fifth Avenue, Manhattan', 'image_id': '482716', 'multi': True, 'sequence_number': 1, 'high_res_link': 'http://link.nypl...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_where_for_what('https://digitalcollections.nypl.org/collections/changing-new-york', \"salmagundi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index.html?tm=1553505375349\" is collected succesfully.\n",
      "\n",
      "Unique match is found:\n",
      "<h1>清上海书业商团旗帜</h1>\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>清上海书业商团旗帜</td>\n",
       "      <td>/historymuseum/historymuseum/dc/myyp/2020/01/23/3419955b6e5e35b7016fd28a7f890c42.html?tm=1579785391841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>民国徐汇公学教具--徐汇中学捐赠</td>\n",
       "      <td>/historymuseum/historymuseum/dc/myyp/2019/12/18/3419955b6e5e35b7016f17d0fb000572.html?tm=1579785391841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1934年上海徐家汇土山湾铸铜钟</td>\n",
       "      <td>/historymuseum/historymuseum/dc/myyp/2019/11/19/3419955b6e5e35b7016e8165260d01b8.html?tm=1579785391841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1907年外白渡桥落成铭牌</td>\n",
       "      <td>/historymuseum/historymuseum/dc/myyp/2019/10/16/3419955b6dc7fb5a016dd2e36ae000fd.html?tm=1579785391841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               text  \\\n",
       "0         清上海书业商团旗帜   \n",
       "1  民国徐汇公学教具--徐汇中学捐赠   \n",
       "2  1934年上海徐家汇土山湾铸铜钟   \n",
       "3     1907年外白渡桥落成铭牌   \n",
       "\n",
       "                                                                                                      url  \n",
       "0  /historymuseum/historymuseum/dc/myyp/2020/01/23/3419955b6e5e35b7016fd28a7f890c42.html?tm=1579785391841  \n",
       "1  /historymuseum/historymuseum/dc/myyp/2019/12/18/3419955b6e5e35b7016f17d0fb000572.html?tm=1579785391841  \n",
       "2  /historymuseum/historymuseum/dc/myyp/2019/11/19/3419955b6e5e35b7016e8165260d01b8.html?tm=1579785391841  \n",
       "3  /historymuseum/historymuseum/dc/myyp/2019/10/16/3419955b6dc7fb5a016dd2e36ae000fd.html?tm=1579785391841  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_where_for_what('http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index.html?tm=1553505375349', \"清上海书业商\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7\" is collected succesfully.\n",
      "\n",
      "There are 0 matched elements given your last input. They are:\n",
      "\n",
      "\n",
      "\n",
      "What is the displayed text for one of the elements you want to scrape: \n",
      "126\n",
      "\n",
      "There are 0 matched elements given your last input. They are:\n",
      "\n",
      "\n",
      "\n",
      "What is the displayed text for one of the elements you want to scrape: \n",
      "136\n",
      "\n",
      "There are 0 matched elements given your last input. They are:\n",
      "\n",
      "\n",
      "\n",
      "What is the displayed text for one of the elements you want to scrape: \n",
      "158\n",
      "\n",
      "There are 0 matched elements given your last input. They are:\n",
      "\n",
      "\n",
      "\n",
      "What is the displayed text for one of the elements you want to scrape: (Type \"QUIT\" to stop)\n",
      "QUIT\n",
      "\n",
      "[Error] It is very likely that the website is not fully collected.\n",
      "        Please try this command: scrape_where_for_what(YOUR_URL, YOUR_TARGET_PHRASE, save_as_html = True)\n",
      "        A HTML file will be created in your local folder, open it with a browser.\n",
      "        If you cannot see what you want to find on the page, please switch to dynamic scraping method.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_where_for_what('https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7', \"2013-12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7\" is collected succesfully.\n",
      "[Success] The HTML file is saved succesfully.\n",
      "\n",
      "There are 0 matched elements given your last input. They are:\n",
      "\n",
      "\n",
      "\n",
      "What is the displayed text for one of the elements you want to scrape: \n",
      "QUIT\n",
      "\n",
      "[Error] It is very likely that the website is not fully collected.\n",
      "        Please try this command: scrape_where_for_what(YOUR_URL, YOUR_TARGET_PHRASE, save_as_html = True)\n",
      "        A HTML file will be created in your local folder, open it with a browser.\n",
      "        If you cannot see what you want to find on the page, please switch to dynamic scraping method.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_where_for_what('https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7', \"2013-12\", save_as_html=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once the content of the website is collected, the next step is to parse the page. \n",
    "# After parsing, we can find the elements we want, then extract and clean their values. \n",
    "# For this step, there are many choices of libraries, some examples are: \n",
    "\n",
    "#  - BeautifulSoup\n",
    "#  - Scrapy\n",
    "#  - Lxml\n",
    "#  - AdvancedHTMLParser\n",
    " \n",
    "# In this pipeline, we will explore `BeautifulSoup`. `BeautifulSoup` and `Scrapy` are two popular scraping tools. Between these two, BeautifulSoup is more user-friendly, while Scrapy is more efficient and scalable. \n",
    "# As this pipeline is targeted for people with less technical backgrounds, we will sacrafice some efficiency for more intuitive experience.\n",
    "# If you are working on a large-scale or high-velocity scraping project, please consider Scrapy or other tool.\n",
    "# The other two libraries listed above are good choices in their specific areas, so keep them in view:\n",
    "#  - Lxml has rich features for processing XML and HTML and is quite efficient (BeautifulSoup actually supports using Lxml parser among other parsers).\n",
    "#  - AdvancedHTMLParser has similar functions like in native JavaScript and supports complex operations on HTML.\n",
    "  \n",
    "### Choices of Parsers\n",
    "# References:\n",
    "#  - https://smartproxy.com/blog/scrapy-vs-beautifulsoup (Use cases comparison and Pros&Cons)\n",
    "#  - https://tomassetti.me/parsing-html (Common libraries in different programming languages)\n",
    "#  - https://medium.com/analytics-vidhya/scrapy-vs-selenium-vs-beautiful-soup-for-web-scraping-24008b6c87b8 (Great comparison article that includes Selenium, which is the popular choice for dynamic website scraping)\n",
    "\n",
    "### Beautiful Soup\n",
    "# References:   \n",
    "#  - https://www.datacamp.com/community/tutorials/amazon-web-scraping-using-beautifulsoup (Showed how to write element finding logic in hierarchy)\n",
    "#  - https://stackabuse.com/guide-to-parsing-html-with-beautifulsoup-in-python (Nice illustrations, browse_and_scrape combines pagination with parsing) \n",
    "#  - https://www.crummy.com/software/BeautifulSoup/bs4/doc (Long but detailed description of BS4 usage)\n",
    "#  - https://www.crummy.com/software/BeautifulSoup (The \"Hall of Fame\" section has some high-profile projects, worth having a look)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # $$$\n",
    "# # To be implemented:\n",
    "\n",
    "# An interesting side note, here's one quote from the BS project page:\n",
    "# > You can tell it \"Find all the links\", or \"Find all the links of class externalLink\", or \"Find all the links whose urls match \"foo.com\", or \"Find the table heading that's got bold text, then give me that text.\"\n",
    "# But actually, you CANNOT directly ask BS these natural language questions. You need to write codes that follow the syntax of the BS4 library, which is similar but not quite close to natural language. \n",
    "# **Programming with natural language** is one of the directions worth pursuing in the future, as it further lowers the bar for utilizing web scraping and related technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # $$$\n",
    "# # To be implemented:\n",
    "\n",
    "# Note the requests made by the `get_response` function might be recognized as robotic access by some website. \n",
    "# To bypass screening by those websites, additional specifications on headers and proxies are required. \n",
    "# These additional setup will be implemented in the future versions.\n",
    "\n",
    "# As a reference, the `get` function from the Python library requests takes the following parameters:\n",
    "# - url – URL for the new  Request object.\n",
    "# - params – (optional) Dictionary of GET Parameters to send with the Request.\n",
    "# - headers – (optional) Dictionary of HTTP Headers to send with the Request.\n",
    "# - cookies – (optional) CookieJar object to send with the  Request.\n",
    "# - auth – (optional) AuthObject to enable Basic HTTP Auth.\n",
    "# - timeout – (optional) Float describing the timeout of the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # $$$\n",
    "# # To be integrated:\n",
    "# # When getting the response, use unicode-dammit to detect encodings in smart ways (https://www.crummy.com/software/BeautifulSoup/bs4/doc/#unicode-dammit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # $$$\n",
    "# # To be implemented formally: Get next/prev sibling\n",
    "# for child in get_contents(sample_element.parent): # [::-1]\n",
    "#     print(child == sample_element) # if true then next is next sib or prev sib depending on how contents list is ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # $$$\n",
    "# # Ready have open-source implementation, but :\n",
    "# # css_selector_to_xpath is getting a result that is too complicated\n",
    "# # xpath_to_css_selector cannot handle slightly more complex xpath\n",
    "# # For simple xpath-css conversion, own implementation might be more transparent and reliable\n",
    "\n",
    "# # !pip3 install cssify\n",
    "# # !pip3 install cssselect\n",
    "\n",
    "# # Reference: https://github.com/santiycr/cssify\n",
    "# from cssify import cssify\n",
    "# def xpath_to_css_selector(xpath_string):\n",
    "#     return cssify(xpath_string)\n",
    "\n",
    "# # Reference: https://lxml.de/cssselect.html\n",
    "# from cssselect import GenericTranslator\n",
    "# def css_selector_to_xpath(css_selector_string):\n",
    "#     return GenericTranslator().css_to_xpath()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # $$$\n",
    "# # Implemented as a solution to indicate to the user which parts of the websites will be scraped\n",
    "# # However, this involves opening up the newly created html file in a browser and searching for changes\n",
    "# # The highlighting may not be obvious and the process is almost as complex as using broswer inspector\n",
    "# # Thus this functionality is dropped and should be kept in view (KIV)\n",
    "\n",
    "# def highlight_element(element, highlight_style = \"background-color: rgba(255,0,0,0.5); border: 3px dotted yellow\"):\n",
    "#     element['style'] = highlight_style\n",
    "\n",
    "# def highlight_elements(elements, highlight_style = \"background-color: rgba(255,0,0,0.5); border: 3px dotted yellow; \"):\n",
    "#     for element in elements:\n",
    "#         element['style'] = highlight_style\n",
    "\n",
    "# # highlighted_soup = highlight_element(soup.select(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# # Reference: How to fake and rotate User Agents using Python 3\n",
    "# # https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "# import requests\n",
    "# import random \n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # This data was created by using the curl method explained above\n",
    "# headers_list = [\n",
    "#     # Firefox 77 Mac\n",
    "#      {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\"\n",
    "#     },\n",
    "#     # Firefox 77 Windows\n",
    "#     {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\"\n",
    "#     },\n",
    "#     # Chrome 83 Mac\n",
    "#     {\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "#         \"Sec-Fetch-Site\": \"none\",\n",
    "#         \"Sec-Fetch-Mode\": \"navigate\",\n",
    "#         \"Sec-Fetch-Dest\": \"document\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "#     },\n",
    "#     # Chrome 83 Windows \n",
    "#     {\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "#         \"Sec-Fetch-Site\": \"same-origin\",\n",
    "#         \"Sec-Fetch-Mode\": \"navigate\",\n",
    "#         \"Sec-Fetch-User\": \"?1\",\n",
    "#         \"Sec-Fetch-Dest\": \"document\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "#     }\n",
    "# ]\n",
    "# # Create ordered dict from Headers above\n",
    "# ordered_headers_list = []\n",
    "# for headers in headers_list:\n",
    "#     h = OrderedDict()\n",
    "#     for header,value in headers.items():\n",
    "#         h[header]=value\n",
    "#     ordered_headers_list.append(h)\n",
    "    \n",
    "    \n",
    "# url = 'https://httpbin.org/headers'\n",
    "\n",
    "# for i in range(1,4):\n",
    "#     #Pick a random browser headers\n",
    "#     headers = random.choice(headers_list)\n",
    "#     #Create a request session\n",
    "#     r = requests.Session()\n",
    "#     r.headers = headers\n",
    "    \n",
    "#     response = r.get(url)\n",
    "#     print(\"Request #%d\\nUser-Agent Sent:%s\\n\\nHeaders Recevied by HTTPBin:\"%(i,headers['User-Agent']))\n",
    "#     print(response.json())\n",
    "#     print(\"-------------------\")\n",
    "\n",
    "# ################################################################################\n",
    "# # Reference: How To Rotate Proxies and change IP Addresses using Python 3\n",
    "# # https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/    \n",
    "    \n",
    "# from lxml.html import fromstring\n",
    "# import requests\n",
    "# from itertools import cycle\n",
    "# import traceback\n",
    "\n",
    "# def get_proxies():\n",
    "#     url = 'https://free-proxy-list.net/'\n",
    "#     response = requests.get(url)\n",
    "#     parser = fromstring(response.text)\n",
    "#     proxies = set()\n",
    "#     for i in parser.xpath('//tbody/tr')[:10]:\n",
    "#         if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "#             proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "#             proxies.add(proxy)\n",
    "#     return proxies\n",
    "\n",
    "\n",
    "# #If you are copy pasting proxy ips, put in the list below\n",
    "# #proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "# proxies = get_proxies()\n",
    "# proxy_pool = cycle(proxies)\n",
    "\n",
    "# url = 'https://httpbin.org/ip'\n",
    "# for i in range(1,11):\n",
    "#     #Get a proxy from the pool\n",
    "#     proxy = next(proxy_pool)\n",
    "#     print(\"Request #%d\"%i)\n",
    "#     try:\n",
    "#         response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "#         print(response.json())\n",
    "#     except:\n",
    "#         #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "#         #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "#         print(\"Skipping. Connnection error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
