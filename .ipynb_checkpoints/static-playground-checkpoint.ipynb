{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This is a playground for developing the static website scraper. Functions developed here will be eventually packaged into .py files.\n",
    "\n",
    "\n",
    "**Progress**\n",
    "\n",
    "Done:\n",
    "    1. get response function\n",
    "    2. element finder and extractor (parser)\n",
    "    3. file saver\n",
    "\n",
    "To-do:\n",
    "    4. pagination\n",
    "    5. rotate agent\n",
    "    6. rotate IP\n",
    "    \n",
    "Long-term:\n",
    "    - support authentication \n",
    "    - support cookie\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Make sure you install the required libraries\n",
    "# !pip3 install --upgrade requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key libraries\n",
    "import requests\n",
    "from requests.exceptions import HTTPError\n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions help us understand the variables that exist in the environment\n",
    "# which is useful for creating natural language interface for data analysis\n",
    "\n",
    "import inspect\n",
    "def get_local_variables(ignore_underscore = True):\n",
    "    \"\"\"Get the name and definition of the local variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dictionary\n",
    "        A mapping between name and definition of the local variables.\n",
    "                \n",
    "    \"\"\"\n",
    "    callers_local_vars = dict(inspect.currentframe().f_back.f_locals.items())\n",
    "    if filter_:\n",
    "        var_keys = list(callers_local_vars.keys())\n",
    "        for key in var_keys:\n",
    "            if key.startswith('_'):\n",
    "                del callers_local_vars[key]\n",
    "    return callers_local_vars\n",
    "def retrieve_name(var):\n",
    "    \"\"\"Retrieve the name of the variable. # Reference https://stackoverflow.com/a/40536047.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var: object \n",
    "        Variable to get the name of.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        Name of the variable passed.\n",
    "        \n",
    "    \"\"\"\n",
    "    for fi in reversed(inspect.stack()):\n",
    "        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "        if len(names) > 0:\n",
    "            return names[0]\n",
    "        \n",
    "def get_attributes(obj, ignore_underscore = True):\n",
    "    \"\"\"Get a list of valid attributes of the object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A list of valid attributes of the object.\n",
    "                \n",
    "    \"\"\"\n",
    "    return [x for x in dir(obj) if not x.startswith('_')]\n",
    "\n",
    "def print_attributes_and_values(obj, ignore_underscore = True):\n",
    "    \"\"\"Print the valid attributes of the object and their corresponding values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "                \n",
    "    \"\"\"\n",
    "    obj_name = retrieve_name(obj)\n",
    "    attributes = get_attributes(obj, ignore_underscore = ignore_underscore)\n",
    "    for attr in attributes:\n",
    "        obj_attr_string = obj_name+'.'+attr\n",
    "        print(obj_attr_string)\n",
    "        print(' '*4 + str(eval(obj_attr_string))[:60])\n",
    "        print('-'*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: get response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `requests.get` function takes the following parameters:\n",
    "\n",
    " - url – URL for the new  Request object.\n",
    " - params – (optional) Dictionary of GET Parameters to send with the Request.\n",
    " - headers – (optional) Dictionary of HTTP Headers to send with the Request.\n",
    " - cookies – (optional) CookieJar object to send with the  Request.\n",
    " - auth – (optional) AuthObject to enable Basic HTTP Auth.\n",
    " - timeout – (optional) Float describing the timeout of the request.\n",
    " \n",
    "Note the requests made by this function could be recognized as robotic access by some website. To bypass screening by those websites, additional specifications on headers and proxies are required. These additional setup will be implemented in the future versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multipledispatch import dispatch\n",
    "\n",
    "@dispatch(str)\n",
    "def get_response(url, verbose = True):\n",
    "    \"\"\"Get the response of the HTTP GET request for the target url.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        The url to the website that needs to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    response object\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status() # Raise Exception when response was not successful\n",
    "    except HTTPError as http_err:\n",
    "        print('[Error] HTTP error occurred: '+str(http_err))\n",
    "        return requests.models.Response() # Return empty response\n",
    "    except Exception as err:\n",
    "        print('[Error] Other error occurred: '+str(err))\n",
    "        return requests.models.Response() # Return empty response\n",
    "    else:\n",
    "        if verbose:\n",
    "            print('[Success] The website at \"'+url+'\" is collected succesfully.')\n",
    "        return response\n",
    "\n",
    "@dispatch(list)\n",
    "def get_response(urls, verbose = True):\n",
    "    \"\"\"Get the responses of the HTTP GET requests for the target urls. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    urls: list of string\n",
    "        The urls to the websites that need to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of response object\n",
    "        \n",
    "    \"\"\"\n",
    "    return [get_response(url) for url in urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Success and failure examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"http://google.com\" is collected succesfully.\n"
     ]
    }
   ],
   "source": [
    "response = get_response('http://google.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Error] Other error occurred: HTTPConnectionPool(host='asdasdfasdfa.com', port=80): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x10a502198>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))\n"
     ]
    }
   ],
   "source": [
    "response = get_response('http://asdasdfasdfa.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"http://google.com\" is collected succesfully.\n",
      "[Success] The website at \"https://baidu.com\" is collected succesfully.\n",
      "[Error] Other error occurred: HTTPSConnectionPool(host='xasdfxxas.com', port=443): Max retries exceeded with url: / (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x10a50e6d8>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known',))\n"
     ]
    }
   ],
   "source": [
    "response_list = get_response(['http://google.com','https://baidu.com','https://xasdfxxas.com'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example to use throughout this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://cityrecord.engineering.nyu.edu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Success] The website at \"http://cityrecord.engineering.nyu.edu\" is collected succesfully.\n"
     ]
    }
   ],
   "source": [
    "response = get_response(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: parse response function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the content of the website is collected, the next step is to parse the page. After parsing, we can find the elements we want, then extract and clean their values. For this step, there are many choices of libraries, some examples are: \n",
    "\n",
    " - BeautifulSoup\n",
    " - Scrapy\n",
    " - Lxml\n",
    " - AdvancedHTMLParser\n",
    " \n",
    "In this pipeline, we will explore `BeautifulSoup` and `Scrapy` for parsing, because these two are relatively more popular and have rich resources. Between these two, BeautifulSoup is more user-friendly, while Scrapy is more efficient and scalable.\n",
    "\n",
    "The other two libraries listed above are good choices in their specific areas, so keep them in view:\n",
    " - Lxml has rich features for processing XML and HTML and is quite efficient.\n",
    " - AdvancedHTMLParser has similar functions like in native JavaScript and supports complex operations on HTML.\n",
    "  \n",
    "References:\n",
    " - https://smartproxy.com/blog/scrapy-vs-beautifulsoup (Use cases comparison and Pros&Cons)\n",
    " - https://tomassetti.me/parsing-html (Common libraries in different programming languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrapy\n",
    "\n",
    "References:   \n",
    " - https://docs.scrapy.org/en/latest/intro/tutorial.html\n",
    " - https://www.jitsejan.com/using-scrapy-in-jupyter-notebook.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: file saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # filetype = url.split('.')[-1]\n",
    "# filetype = 'html'\n",
    "# filename = re.sub(r'^https?://','',url).replace('.','-').replace('/','_') + '.' + filetype\n",
    "\n",
    "# with open(filename,'w') as f:\n",
    "#     f.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources for To-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ################################################################################\n",
    "# # Reference: How to fake and rotate User Agents using Python 3\n",
    "# # https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3/\n",
    "\n",
    "# import requests\n",
    "# import random \n",
    "# from collections import OrderedDict\n",
    "\n",
    "# # This data was created by using the curl method explained above\n",
    "# headers_list = [\n",
    "#     # Firefox 77 Mac\n",
    "#      {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\"\n",
    "#     },\n",
    "#     # Firefox 77 Windows\n",
    "#     {\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\"\n",
    "#     },\n",
    "#     # Chrome 83 Mac\n",
    "#     {\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"DNT\": \"1\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "#         \"Sec-Fetch-Site\": \"none\",\n",
    "#         \"Sec-Fetch-Mode\": \"navigate\",\n",
    "#         \"Sec-Fetch-Dest\": \"document\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\"\n",
    "#     },\n",
    "#     # Chrome 83 Windows \n",
    "#     {\n",
    "#         \"Connection\": \"keep-alive\",\n",
    "#         \"Upgrade-Insecure-Requests\": \"1\",\n",
    "#         \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "#         \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\",\n",
    "#         \"Sec-Fetch-Site\": \"same-origin\",\n",
    "#         \"Sec-Fetch-Mode\": \"navigate\",\n",
    "#         \"Sec-Fetch-User\": \"?1\",\n",
    "#         \"Sec-Fetch-Dest\": \"document\",\n",
    "#         \"Referer\": \"https://www.google.com/\",\n",
    "#         \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "#         \"Accept-Language\": \"en-US,en;q=0.9\"\n",
    "#     }\n",
    "# ]\n",
    "# # Create ordered dict from Headers above\n",
    "# ordered_headers_list = []\n",
    "# for headers in headers_list:\n",
    "#     h = OrderedDict()\n",
    "#     for header,value in headers.items():\n",
    "#         h[header]=value\n",
    "#     ordered_headers_list.append(h)\n",
    "    \n",
    "    \n",
    "# url = 'https://httpbin.org/headers'\n",
    "\n",
    "# for i in range(1,4):\n",
    "#     #Pick a random browser headers\n",
    "#     headers = random.choice(headers_list)\n",
    "#     #Create a request session\n",
    "#     r = requests.Session()\n",
    "#     r.headers = headers\n",
    "    \n",
    "#     response = r.get(url)\n",
    "#     print(\"Request #%d\\nUser-Agent Sent:%s\\n\\nHeaders Recevied by HTTPBin:\"%(i,headers['User-Agent']))\n",
    "#     print(response.json())\n",
    "#     print(\"-------------------\")\n",
    "\n",
    "# ################################################################################\n",
    "# # Reference: How To Rotate Proxies and change IP Addresses using Python 3\n",
    "# # https://www.scrapehero.com/how-to-rotate-proxies-and-ip-addresses-using-python-3/    \n",
    "    \n",
    "# from lxml.html import fromstring\n",
    "# import requests\n",
    "# from itertools import cycle\n",
    "# import traceback\n",
    "\n",
    "# def get_proxies():\n",
    "#     url = 'https://free-proxy-list.net/'\n",
    "#     response = requests.get(url)\n",
    "#     parser = fromstring(response.text)\n",
    "#     proxies = set()\n",
    "#     for i in parser.xpath('//tbody/tr')[:10]:\n",
    "#         if i.xpath('.//td[7][contains(text(),\"yes\")]'):\n",
    "#             proxy = \":\".join([i.xpath('.//td[1]/text()')[0], i.xpath('.//td[2]/text()')[0]])\n",
    "#             proxies.add(proxy)\n",
    "#     return proxies\n",
    "\n",
    "\n",
    "# #If you are copy pasting proxy ips, put in the list below\n",
    "# #proxies = ['121.129.127.209:80', '124.41.215.238:45169', '185.93.3.123:8080', '194.182.64.67:3128', '106.0.38.174:8080', '163.172.175.210:3128', '13.92.196.150:8080']\n",
    "# proxies = get_proxies()\n",
    "# proxy_pool = cycle(proxies)\n",
    "\n",
    "# url = 'https://httpbin.org/ip'\n",
    "# for i in range(1,11):\n",
    "#     #Get a proxy from the pool\n",
    "#     proxy = next(proxy_pool)\n",
    "#     print(\"Request #%d\"%i)\n",
    "#     try:\n",
    "#         response = requests.get(url,proxies={\"http\": proxy, \"https\": proxy})\n",
    "#         print(response.json())\n",
    "#     except:\n",
    "#         #Most free proxies will often get connection errors. You will have retry the entire request using another proxy to work. \n",
    "#         #We will just skip retries as its beyond the scope of this tutorial and we are only downloading a single url \n",
    "#         print(\"Skipping. Connnection error\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
