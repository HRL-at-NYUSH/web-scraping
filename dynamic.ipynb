{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introduction**\n",
    "\n",
    "This is a develop environment for the dynamic/static website scraper. Functions developed here will be eventually packaged into .py files and called from other notebook or python scripts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Please install the required Python libraries\n",
    "\n",
    "# !pip3 install --upgrade pandas # library for manipulating structured data\n",
    "# !pip3 install --upgrade numpy # library for fundamentals of array computing\n",
    "# !pip3 install --upgrade requests # library for making request for the static websites\n",
    "# !pip3 install --upgrade soupsieve  # library to support css selector in beautifulsoup\n",
    "# !pip3 install --upgrade beautifulsoup4 # a parser that balances between efficiency and leniency\n",
    "# !pip3 install --upgrade --user lxml # a more efficient parser\n",
    "# !pip3 install --upgrade html5lib # a parser that acts like a browser, most lenient\n",
    "# !pip3 install --upgrade webdriver-manager # library that helps user manage the installation and usage of web drivers\n",
    "# !pip3 install selenium-wire==3.0.6 # library for automating web browser interaction, extended to inspect requests\n",
    "\n",
    "# Basic libraries\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import inspect\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "# Web scraping related libraries\n",
    "import requests\n",
    "import bs4\n",
    "from seleniumwire import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "from collections import OrderedDict\n",
    "from collections.abc import Iterable\n",
    "\n",
    "def ordered_remove_duplicates(li):\n",
    "    \"\"\"Remove duplicates and arrange the unique elements by the order of their first occurences.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    li: list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        \n",
    "    \"\"\"\n",
    "    return list(OrderedDict.fromkeys(li))\n",
    "\n",
    "def remove_blank_element_in_list(li):\n",
    "    \"\"\"Return a cleaned version of the list with all blank elements removed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    li: list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        \n",
    "    \"\"\"\n",
    "    return [element for element in li if element.strip()!='']\n",
    "\n",
    "def flatten_list(l):\n",
    "    \"\"\"Flatten a list of lists to a one-layer list (elements are in original order). Note this is NOT recursive, meaning multi-layered list of lists cannot be converted into a single-layered list in one transformation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    l: list\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "def is_iterable(obj):\n",
    "    \"\"\"Check if the passed object is iterable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obj: object\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return isinstance(obj, Iterable)\n",
    "\n",
    "##################################################################################################\n",
    "# These functions help us understand the variables that exist in the environment\n",
    "# which is useful for creating natural language interface for data analysis\n",
    "\n",
    "def get_local_variables(ignore_underscore = True):\n",
    "    \"\"\"Get the name and definition of the local variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dictionary\n",
    "        A mapping between name and definition of the local variables.\n",
    "                \n",
    "    \"\"\"\n",
    "    callers_local_vars = dict(inspect.currentframe().f_back.f_locals.items())\n",
    "    if ignore_underscore:\n",
    "        var_keys = list(callers_local_vars.keys())\n",
    "        for key in var_keys:\n",
    "            if key.startswith('_'):\n",
    "                del callers_local_vars[key]\n",
    "    return callers_local_vars\n",
    "\n",
    "def get_global_variables(ignore_underscore = True):\n",
    "    \"\"\"Get the name and definition of the global variables.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dictionary\n",
    "        A mapping between name and definition of the global variables.\n",
    "                \n",
    "    \"\"\"\n",
    "    callers_global_vars = dict(inspect.currentframe().f_back.f_globals.items())\n",
    "    if ignore_underscore:\n",
    "        var_keys = list(callers_global_vars.keys())\n",
    "        for key in var_keys:\n",
    "            if key.startswith('_'):\n",
    "                del callers_global_vars[key]\n",
    "    return callers_global_vars\n",
    "\n",
    "def retrieve_name(var):\n",
    "    \"\"\"Retrieve the name of the variable. # Reference https://stackoverflow.com/a/40536047.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    var: object \n",
    "        Variable to get the name of.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        Name of the variable passed.\n",
    "        \n",
    "    \"\"\"\n",
    "    for fi in reversed(inspect.stack()):\n",
    "        names = [var_name for var_name, var_val in fi.frame.f_locals.items() if var_val is var]\n",
    "        if len(names) > 0:\n",
    "            return names[0]\n",
    "        \n",
    "def get_attributes(obj, ignore_underscore = True):\n",
    "    \"\"\"Get a list of valid attributes of the object.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list\n",
    "        A list of valid attributes of the object.\n",
    "                \n",
    "    \"\"\"\n",
    "    return [x for x in dir(obj) if not x.startswith('_')]\n",
    "\n",
    "def print_attributes_and_values(obj, ignore_underscore = True):\n",
    "    \"\"\"Print the valid attributes of the object and their corresponding values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ignore_underscore : boolean (optional, default = True)\n",
    "        Whether or not the variables starting with \"_\" need to be filtered out.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "                \n",
    "    \"\"\"\n",
    "    obj_name = retrieve_name(obj)\n",
    "    attributes = get_attributes(obj, ignore_underscore = ignore_underscore)\n",
    "    for attr in attributes:\n",
    "        obj_attr_string = obj_name+'.'+attr\n",
    "        print(obj_attr_string)\n",
    "        print(' '*4 + str(eval(obj_attr_string))[:60])\n",
    "        print('-'*70)\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def is_readable_content(content):\n",
    "    \"\"\"Return whether the content passed is a readable content like Tag or NavigableString; not CData, Comment, Declaration, Doctype, ProcessingInstruction, ResultSet, Script, Stylesheet, XMLFormatter.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    # Types that are instances of NavigableString:  CData, Comment, Declaration, Doctype, PreformattedString, ProcessingInstruction, ResultSet, Script, Stylesheet, TemplateString, XMLFormatter\n",
    "    # Types in the group above that are not String:  CData, Comment, Declaration, Doctype, ProcessingInstruction, ResultSet, Script, Stylesheet, XMLFormatter\n",
    "    return isinstance(content, (bs4.element.Tag, bs4.element.NavigableString)) and not isinstance(content, (bs4.element.CData, bs4.element.Comment, bs4.element.Declaration, bs4.element.Doctype, bs4.element.ProcessingInstruction, bs4.element.ResultSet, bs4.element.Script, bs4.element.Stylesheet, bs4.element.XMLFormatter))\n",
    "\n",
    "def get_contents(element):\n",
    "    \n",
    "    \"\"\"Return a list of non-empty and readable contents/children of the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of bs4.element\n",
    "        \n",
    "    \"\"\"\n",
    "    return [content for content in element.contents if str(content).strip()!='' and is_readable_content(content)]\n",
    "\n",
    "def get_contents_names(element):\n",
    "    \"\"\"Return the list of names of the non-empty and readable contents/children of the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    content: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    return [content.name for content in get_contents(element)]\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def get_response(url, verbose = True, driver = None ):\n",
    "    \"\"\"Get the response of the HTTP GET request for the target url.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        The url to the website that needs to be scraped. \n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not [Success] message should be printed.\n",
    "    driver: seleniumwire.webdriver.browser.Chrome (optional, default = None)\n",
    "        The web browser driver with which to get the page source of dynamic website.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    response object or string\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Static mode when driver is None\n",
    "    if driver is None:\n",
    "        \n",
    "        headers_list = [{'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer': 'https://www.google.com/', 'DNT': '1', 'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1'}, {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:77.0) Gecko/20100101 Firefox/77.0', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8', 'Referer': 'https://www.google.com/', 'DNT': '1', 'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1'}, {'Connection': 'keep-alive', 'DNT': '1', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'Sec-Fetch-Site': 'none', 'Sec-Fetch-Mode': 'navigate', 'Sec-Fetch-Dest': 'document', 'Referer': 'https://www.google.com/'}, {'Connection': 'keep-alive', 'Upgrade-Insecure-Requests': '1', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36', 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9', 'Sec-Fetch-Site': 'same-origin', 'Sec-Fetch-Mode': 'navigate', 'Sec-Fetch-User': '?1', 'Sec-Fetch-Dest': 'document', 'Referer': 'https://www.google.com/'}] # Reference: https://www.scrapehero.com/how-to-fake-and-rotate-user-agents-using-python-3    \n",
    "\n",
    "        try:\n",
    "            headers = random.choice(headers_list)\n",
    "            response = requests.get(url, headers = headers)\n",
    "            response.raise_for_status() # Raise Exception when response was not successful\n",
    "        except requests.exceptions.HTTPError as http_err:\n",
    "            print('[Error] HTTP error occurred: '+str(http_err))\n",
    "            return requests.models.Response() # Return empty response\n",
    "        except Exception as err:\n",
    "            print('[Error] Other error occurred: '+str(err))\n",
    "            return requests.models.Response() # Return empty response\n",
    "        else:\n",
    "            if verbose:\n",
    "                print('[Success] The website at \"'+url+'\" is collected successfully.')\n",
    "            return response\n",
    "    \n",
    "    # Dynamic mode when driver is provided\n",
    "    else:\n",
    "        \n",
    "        if not is_driver_at_url(driver, url):\n",
    "            go_to_page(driver, url)\n",
    "            scroll_to_bottom(driver)\n",
    "\n",
    "        return get_page_source(driver)\n",
    "    \n",
    "        \n",
    "def get_soup(response, default_parser = 'lxml'):\n",
    "    \"\"\"Get the beautiful soup object of the response object or filepath or html string.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    response: requests.models.Response, string\n",
    "        The response object or filepath or html string. \n",
    "    default_parser: string (optional, default = lxml)\n",
    "        Which parser to use when parsing the response.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of response object\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(response, requests.models.Response):\n",
    "        soup = bs4.BeautifulSoup(response.content, default_parser)\n",
    "    else:\n",
    "        try:\n",
    "            soup = bs4.BeautifulSoup(response, default_parser)\n",
    "        except Exception as err:\n",
    "            print('[Error] The response object you provided cannot be turned into beautiful soup object: '+str(err))\n",
    "    return soup\n",
    "\n",
    "def save_html(html_object, url , path = ''):\n",
    "    \"\"\"Save the response or soup object as a HTML file at the path provided.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    html_object: requests.models.Response, bs4.BeautifulSoup\n",
    "        The response or soup object. \n",
    "    path: string (optional, default = ./TEMP.html)\n",
    "        The path at which the HTML file will be saved.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "        \n",
    "    \"\"\"\n",
    "    if path == '':\n",
    "        path = './'+re.sub('^https?://','',url).replace('/','_').replace('.','-')+'.html'\n",
    "    if isinstance(html_object, requests.models.Response):\n",
    "        html_text = html_object.text\n",
    "    elif isinstance(html_object, (bs4.BeautifulSoup,bs4.element.Tag)):\n",
    "        html_text = str(html_object.prettify())\n",
    "    else:\n",
    "        html_text = str(html_object)\n",
    "    try:\n",
    "        with open(path,'w') as f:\n",
    "            f.write(html_text)\n",
    "            print('[Success] The HTML file is saved succesfully.')\n",
    "    except Exception as err:\n",
    "        print('[Error] The response object you provided cannot be turned into beautiful soup object: '+str(err))\n",
    "\n",
    "def get_response_and_save_html(url, driver = None, path = ''):\n",
    "    \"\"\"Get the response of the website and save it as an HTML.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        The url to the website that needs to be scraped. \n",
    "    driver: seleniumwire.webdriver.browser.Chrome (optional, default = None)\n",
    "        The web browser driver with which to get the page source of dynamic website.\n",
    "    path: string (optional, default = ./TEMP.html)\n",
    "        The path at which the HTML file will be saved.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    response = get_response(url, driver = driver)\n",
    "    \n",
    "    save_html(response.text, url, path = path)\n",
    "    \n",
    "##################################################################################################\n",
    "\n",
    "def get_self_index(element):\n",
    "    \"\"\"Return the index of the element among its siblings of the same type.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    int\n",
    "        \n",
    "    \"\"\"\n",
    "    self_type = element.name\n",
    "    previous_siblings_of_all_types = list(element.previous_siblings)\n",
    "    previous_siblings_of_same_type = [element for element in previous_siblings_of_all_types if element.name == self_type]\n",
    "    return len(previous_siblings_of_same_type) + 1 # css selector starts indexing with 1 instead of 0\n",
    "\n",
    "def describe_part_of_css_selector(element):\n",
    "    \"\"\"Construct part of the css selector path.\n",
    "    # Reference: https://stackoverflow.com/a/32263260 (basic structure inspiration)\n",
    "    # Reference: https://csswizardry.com/2012/05/keep-your-css-selectors-short (tips to improve efficiency)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    enough_to_be_unique = False\n",
    "    \n",
    "    element_type = element.name\n",
    "    element_attrs = element.attrs\n",
    "    element_attrs_string = ''\n",
    "    for k,v in element_attrs.items():\n",
    "        if k == 'id':\n",
    "            element_attrs_string += '#' + element_attrs[k]\n",
    "            enough_to_be_unique = True\n",
    "            break\n",
    "        elif k == 'class':\n",
    "            element_attrs_string += '.'+'.'.join(element_attrs[k])\n",
    "\n",
    "    element_part = element_type + element_attrs_string\n",
    "            \n",
    "    if not enough_to_be_unique:\n",
    "        length = get_self_index(element)\n",
    "        if (length) > 1:\n",
    "            element_part = '%s:nth-of-type(%s)' % (element_part, length)\n",
    "        \n",
    "    return element_part\n",
    "\n",
    "def get_css_selector_path(node):\n",
    "    \"\"\"Construct the whole css selector path to a certain element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    node: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    path = [describe_part_of_css_selector(node)]\n",
    "    for parent in node.parents:\n",
    "        if parent.name == 'body' : \n",
    "            break\n",
    "        path.insert(0, describe_part_of_css_selector(parent))\n",
    "    return ' > '.join(path)\n",
    "\n",
    "def elevate_css_selector_path(path):\n",
    "    \"\"\"Get the css selector path to the element that is one level above the current element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        The css selector path to an BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return '>'.join(path.split('>')[:-1]).strip() if '>' in path else path\n",
    "\n",
    "\n",
    "def go_up_multiple_level(orig_path, go_up):\n",
    "    \"\"\"Get the css selector path to the element multiple levels up.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    orig_path: string\n",
    "        The css selector path to the source element.\n",
    "    go_up: int\n",
    "        The number of levels to go up.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    path = orig_path[:]\n",
    "    for i in range(go_up):\n",
    "        path = elevate_css_selector_path(path)\n",
    "    return path\n",
    "\n",
    "def go_up_till_is_tag(element):\n",
    "    \"\"\"Return the nearest Tag element, if not itself, return its parent if it is a Tag element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "        An BS4 element from the parsed tree.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    bs4.element.Tag\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(element, bs4.element.NavigableString):\n",
    "        return element.parent\n",
    "    if isinstance(element, bs4.element.Tag):\n",
    "        return element\n",
    "    else:\n",
    "        print('[Error] Element is still not Tag after getting the parent.')\n",
    "        return None\n",
    "    \n",
    "##################################################################################################\n",
    "\n",
    "def get_directly_related_link(element):\n",
    "    \"\"\"Extract the link directly related to the element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    count = 0\n",
    "    while element.name != 'a' and count < 5:\n",
    "        element = element.parent\n",
    "        if element is None:\n",
    "            return ''\n",
    "        count += 1\n",
    "    if element.name != 'a':\n",
    "        return ''\n",
    "    else:\n",
    "        return element.get('href',default='')\n",
    "\n",
    "def get_indirectly_related_links(element):\n",
    "    \"\"\"Extract the links indirectly related to the element (i.e. belonging to the sibling elements).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return remove_blank_element_in_list([link.get('href',default='') for link in element.parent.find_all('a')])\n",
    "\n",
    "def get_related_link(element):\n",
    "    \"\"\"Extract the link directly related to the element, if none is found, get indirectly related links.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string or list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    link = get_directly_related_link(element)\n",
    "    \n",
    "    if link != '':\n",
    "        return link\n",
    "    else:\n",
    "        links = get_indirectly_related_links(element)\n",
    "        if len(links) == 1 and links[0].strip() != '':\n",
    "            return links[0]\n",
    "        else:\n",
    "            return links\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def extract_text(element):\n",
    "    \"\"\"Extract the textual content of an element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    element: bs4.element\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    return element.getText(separator=u'\\n').strip()\n",
    "\n",
    "def get_longest_separator(text):\n",
    "    \"\"\"Return the longest separator (formed by multiple newline) in the text.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: string\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    if isinstance(text, str) and '\\n' in text:\n",
    "        return max(re.findall(r'\\n+', text, re.DOTALL), key=lambda x: len(x))\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def recursive_split(text):\n",
    "    \"\"\"Return a multi-layer list of lists resulting from a recursive split of the text (split by longer separator first).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text: String\n",
    "        A piece of text that contains separators of different lengths.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    list (of lists)\n",
    "        \n",
    "    \"\"\"\n",
    "    longest_separator = get_longest_separator(text)\n",
    "    if longest_separator == '':\n",
    "        return text\n",
    "    else:\n",
    "        return [recursive_split(part) for part in remove_blank_element_in_list(text.split(longest_separator))]\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def extract_contents(soup, path, verbose = True):\n",
    "    \"\"\"Extract and return the texts and links with the target path in the parsed tree.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup: bs4.soup\n",
    "        The parsed tree of the response.\n",
    "    path: string\n",
    "        The css selector path to the target elements.\n",
    "    verbose: boolean (optional, default = True)\n",
    "        Whether or not to print the process message.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    pd.DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    if soup is None:\n",
    "        return None\n",
    "    \n",
    "    if isinstance(soup, pd.DataFrame):\n",
    "        return soup\n",
    "    \n",
    "    if verbose:\n",
    "        print('\\nExtracting contents ...\\n')\n",
    "    \n",
    "    if path.startswith('HEADER:'):\n",
    "        tables = pd.read_html(str(soup))\n",
    "        target_table = [table for table in tables if str(tuple(table.columns.tolist())) == path.replace('HEADER:','')][0]\n",
    "        return target_table\n",
    "    \n",
    "    target_elements = soup.select(path)\n",
    "\n",
    "    data = pd.DataFrame([(recursive_split(extract_text(target_element)), get_related_link(target_element)) for target_element in target_elements], columns = ['text','url'])\n",
    "\n",
    "    return data\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def get_unique_sample_element(soup, target_phrase = '', context_radius = 40):\n",
    "    \"\"\"Find and return an element based on the html structure and a target phrase, solicit additional information from user through input questions if needed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    soup: bs4.soup\n",
    "        The parsed tree of the response.\n",
    "    target_phrase: string (optional, if not provided, the function will ask user to input)\n",
    "        The phrase used to find the sample element.\n",
    "    context_radius: int (optional, default = 40)\n",
    "        How many characters to display to help user choose recurring phrases based on their contexts.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    bs4.element.Tag\n",
    "        \n",
    "    \"\"\"\n",
    "        \n",
    "    target_phrase = target_phrase.lower()\n",
    "    matched_elements = soup.find_all(text = re.compile(target_phrase,re.IGNORECASE))\n",
    "    attempt_count = 1\n",
    "    \n",
    "    while len(matched_elements)!=1:\n",
    "        \n",
    "        ################################################################\n",
    "        # Situation where matched elements have the same textual content\n",
    "        \n",
    "        if len(set([str(matched_element) for matched_element in matched_elements]))==1:\n",
    "            last_index = -1\n",
    "            phrases_in_context = []\n",
    "            whole_page_text = re.sub('\\s+',' ',soup.text).lower()\n",
    "            \n",
    "            if whole_page_text.count(target_phrase) == len(matched_elements):\n",
    "            \n",
    "                for i in range(whole_page_text.count(target_phrase)):\n",
    "                    current_index = whole_page_text.index(target_phrase,last_index+1)\n",
    "                    phrases_in_context.append(whole_page_text[current_index-context_radius:current_index]+'\\\\\\\\ '+whole_page_text[current_index:current_index+len(target_phrase)]+' //'+whole_page_text[current_index+len(target_phrase):current_index+len(target_phrase)+context_radius])\n",
    "                    last_index = current_index\n",
    "                \n",
    "                if len(set(phrases_in_context))==1:\n",
    "                    print('[Error] There are '+str(len(phrases_in_context))+' occurences of the same target phrase on the page that have very similar contexts.\\nPlease use the browser inspector tool to copy the \"selector\" or \"Selector Path\".\\n')\n",
    "                    return None\n",
    "                else:\n",
    "                    numbered_contexts = ''\n",
    "                    for i in range(len(phrases_in_context)):\n",
    "                        numbered_contexts += 'Choice '+str(i+1)+':  '+phrases_in_context[i] + '\\n'\n",
    "                    print('There are '+str(len(phrases_in_context))+' occurences of the same target phrase on the page,\\nplease choose one based on their contexts:\\n\\n' + numbered_contexts + '\\n')\n",
    "\n",
    "                which_one = 0\n",
    "                while which_one-1 not in range(len(phrases_in_context)):\n",
    "                    which_one = input('Which choice is the element you that want to scrape: [1, 2, 3, ...]\\n')\n",
    "                    try:\n",
    "                        which_one = int(which_one)\n",
    "                    except:\n",
    "                        which_one = 0\n",
    "                matched_elements = [matched_elements[which_one-1]]\n",
    "                \n",
    "            else:\n",
    "                print('[Error] The number of matched elements and the number of target phrase occurences are not the same.\\nPlease use the browser inspector tool to copy the \"selector\" or \"Selector Path\".\\n')\n",
    "                return None\n",
    "            \n",
    "        ###########################################################\n",
    "        if len(matched_elements) > 0 and len(matched_elements) < 5:\n",
    "            # List numbered choices\n",
    "            numbered_choices = ''\n",
    "            for i in range(len(matched_elements)):\n",
    "                numbered_choices += '\\tChoice '+str(i+1)+':  '+str(matched_elements[i])[:80]+ '\\n'\n",
    "\n",
    "            print('\\nThere are '+str(len(matched_elements))+' matched elements given your last input. They are:\\n'+numbered_choices)\n",
    "\n",
    "            # Choose one\n",
    "            which_one = 0\n",
    "            while which_one-1 not in range(len(matched_elements)):\n",
    "                which_one = input('Which choice is the element you that want to scrape: [1, 2, 3, ...]\\n')\n",
    "                try:\n",
    "                    which_one = int(which_one)\n",
    "                except:\n",
    "                    which_one = 0\n",
    "            matched_elements = [matched_elements[which_one-1]]\n",
    "\n",
    "        else:\n",
    "            if len(matched_elements) > 5:\n",
    "                print('\\nThere are '+str(len(matched_elements))+' matched elements given your last input. They are:\\n\\n\\t'+'\\n\\t'.join([str(matched_element)[:80] for matched_element in matched_elements[:10]])+'\\n\\nPlease be more specific in your target phrase.\\n')\n",
    "            if len(matched_elements) == 0:\n",
    "                print('\\nNo match was found, please check for typos in the target phrase (case insensitive) or check if the website is fully collected.')            \n",
    "\n",
    "            # Search again\n",
    "            target_phrase = input('What is the displayed text for one of the elements you want to scrape: '+('(Type \"QUIT\" to stop)' if attempt_count>3 else '')+'\\n')\n",
    "            if target_phrase == 'QUIT':\n",
    "                print('\\n[Error] It is likely that the website is not fully collected.\\n        Please try this command: get_response_and_save_html(PUT_IN_YOUR_URL)\\n        A HTML file will be created in your local folder, open it with a browser.\\n        If you cannot see what you want to find on the page, please switch to dynamic scraping method.\\n')\n",
    "                return None\n",
    "            matched_elements = soup.find_all(text = re.compile(target_phrase,re.IGNORECASE))\n",
    "\n",
    "        # Increment attempt count \n",
    "        attempt_count += 1\n",
    "            \n",
    "    # Match is found by this point\n",
    "    sample_element = matched_elements[0]\n",
    "    sample_element = go_up_till_is_tag(sample_element)\n",
    "    print('\\nUnique match is found:\\n'+str(sample_element)[:100]+ (' ......' if len(str(sample_element))>100 else '') +'\\n\\n')\n",
    "    \n",
    "    # If the sample element is script tag, handle it differently\n",
    "    if sample_element.name == 'script':\n",
    "        matched_lines = [line for line in sample_element.prettify().split('\\n') if target_phrase in line.lower()]\n",
    "        try:\n",
    "            assert(len(matched_lines)==1)\n",
    "            matched_line = matched_lines[0].strip().strip(';')\n",
    "            matched_data = matched_line.split('=',maxsplit=1)[1].strip()\n",
    "            data = pd.DataFrame(json.loads(matched_data))\n",
    "            return data\n",
    "        except:\n",
    "            print('[Error] There are multiple occurences of the target phrase in the JS script.\\nPlease use another more unique target phrase or inspect the page source for the data in JS script.\\n')\n",
    "            return None\n",
    "    \n",
    "    return sample_element\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def scrape_what_from_where(target_phrase, url, driver = None, go_up = 0):\n",
    "    \"\"\"Get the contents that are similar to the element with phrase \"what\" in the website \"where\".\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    target_phrase: string\n",
    "        The displayed text of one of the elements you want to scrape.\n",
    "    url: string\n",
    "        The url of the website you want to scrape.\n",
    "    go_up: int (optional, default = 0)\n",
    "        How many levels to go up in order to get the amount of contents you want.\n",
    "    driver: seleniumwire.webdriver.browser.Chrome (optional, default = None)\n",
    "        The web browser driver with which to get the page source of dynamic website.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    pd.DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    # Get response\n",
    "    response = get_response(url, driver = driver)\n",
    "    \n",
    "    # Get parse tree\n",
    "    soup = get_soup(response)\n",
    "\n",
    "    # Check if the data is in a table, if so, directly return the table\n",
    "    try:\n",
    "        tables = pd.read_html(str(soup))\n",
    "    except:\n",
    "        tables = []\n",
    "\n",
    "    if len(tables)>0 and (len(set([tuple(table.columns.tolist()) for table in tables])) == len(tables)):        \n",
    "        tables_containing_target_phrase = [table for table in tables if target_phrase in str(table)]\n",
    "        tables_containing_target_phrase = sorted(tables_containing_target_phrase, key=lambda t: len(str(t)))\n",
    "        if len(tables_containing_target_phrase)>0:\n",
    "            while len(tables_containing_target_phrase)>0:\n",
    "                print('\\nThere are '+str(len(tables_containing_target_phrase))+' tables with the target phrase:\\n')\n",
    "                target_table = tables_containing_target_phrase[0]\n",
    "                print(target_table)\n",
    "                is_right_table = input('\\nIs this table what you want to scrape? [Yes/No]\\n')\n",
    "                if is_right_table.lower()[0] == 'y':\n",
    "                    right_header = tuple(target_table.columns.tolist())\n",
    "                    print('\\nThe right header is:\\n\\t'+str(right_header))\n",
    "                    return target_table, 'HEADER:'+str(right_header)\n",
    "                else:    \n",
    "                    tables_containing_target_phrase.pop(0)\n",
    "            if len(tables_containing_target_phrase)==0:\n",
    "                print('\\nThe target data is not one of the tables, moving on to other html elements.\\n')\n",
    "\n",
    "    \n",
    "    # Pinpoint the sample element through dialogue\n",
    "    sample_element = get_unique_sample_element(soup, target_phrase)\n",
    "    if sample_element is None:\n",
    "        return None, ''\n",
    "    if isinstance(sample_element, pd.DataFrame):\n",
    "        print('[Success] Data is in the JS script and now extracted as a DataFrame into the variable \"soup\".\\n')\n",
    "        return sample_element, ''\n",
    "    \n",
    "    # Build the css selector path to the sample element\n",
    "    sample_path = get_css_selector_path(sample_element)\n",
    "        \n",
    "    # Go up the parse tree if needed:        \n",
    "    path = go_up_multiple_level(sample_path, go_up = go_up)\n",
    "    \n",
    "    # Extract content\n",
    "    data = extract_contents(soup, path)\n",
    "    \n",
    "    # If data is extracted from html path instead of from json, print the path for future use\n",
    "    if path != '':\n",
    "        print('\\n[Success] The selector path used to extract contents is:\\n\\n\\t'+path+'\\n')\n",
    "    \n",
    "    return data, path\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def create_page_url_list(template_url, start_index, end_index, unique_first_url = None):\n",
    "    \"\"\"Generate a list of urls to scrape from.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    template_url: string\n",
    "        The url template with placeholder \"NUMBER\" that will be replaced by index number.\n",
    "    start_index: int\n",
    "        The first index to be plugged into the template.\n",
    "    end_index: int \n",
    "        The last index to be plugged into the template.\n",
    "    unique_first_url: string (optional, default = None)\n",
    "        If the first web page in the pagination process has a different format compared the one after, provide it here.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    list of string\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    page_url_list = []\n",
    "    if unique_first_url is not None:\n",
    "        page_url_list.append(unique_first_url)\n",
    "    for i in range(start_index,end_index+1):\n",
    "        page_url_list.append(template_url.replace('NUMBER',str(i)))\n",
    "    return page_url_list\n",
    "\n",
    "def get_base_url(url):\n",
    "    \"\"\"Get the base url from a url path.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    url: string\n",
    "        Any url path of the website.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    string\n",
    "        \n",
    "    \"\"\"\n",
    "    return url.split('://')[0]+'://'+url.split('://')[1].split('/')[0]\n",
    "\n",
    "def scrape_path_from_pages(path, pages, driver = None, save_separately = False, file_path_template = None , reporting_interval = None, verbose = False):\n",
    "    \"\"\"Get the contents that are located with path on the pages provided. A batch version of the scrape_what_from_where function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path: string\n",
    "        The css selector path to the target contents.\n",
    "    pages: list of string\n",
    "        The urls to the web pages to be scraped.\n",
    "    driver: seleniumwire.webdriver.browser.Chrome (optional, default = None)\n",
    "        The web browser driver with which to get the page source of dynamic website.    \n",
    "    save_separately: boolean (optional, default = False)\n",
    "        Whether or not to save the data from each page separately.\n",
    "    file_path_template: string (optional, default = None)\n",
    "        If save_separately is True, where to save the data files. It should contain the placeholder \"NUMBER\" which will be replaced by index number.\n",
    "    reporting_interval: int (optional, default = None)\n",
    "        After how many pages should a progress message be printed.\n",
    "    verbose: boolean (optional, default = False)\n",
    "        Whether to print detailed scraping progress messages.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    pd.DataFrame\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    number_of_pages = len(pages)\n",
    "    index_width = len(str(number_of_pages+1))\n",
    "    \n",
    "    if reporting_interval is None:\n",
    "        reporting_interval = int(number_of_pages/10)+1 if number_of_pages<1000 else int(number_of_pages/40)\n",
    "        \n",
    "    output_dataframe = pd.DataFrame()\n",
    "    \n",
    "    for i in range(number_of_pages):\n",
    "        if i % reporting_interval == 0:\n",
    "            print(str(i)+'/'+str(number_of_pages), end=', ')\n",
    "        url = pages[i]\n",
    "        \n",
    "        dataframe = extract_contents(get_soup(get_response(url, verbose = verbose, driver = driver)), path, verbose = verbose)\n",
    "        \n",
    "        if save_separately: \n",
    "            if file_path_template is None:\n",
    "                print('\\n[Error] To save the dataframes from different pages separatorly, you need to provide a file path template.\\n')\n",
    "                return None\n",
    "            file_path = file_path_template.replace('NUMBER', str(i).zfill(index_width))\n",
    "            dataframe.to_csv(file_path, index = False)\n",
    "            \n",
    "        else:\n",
    "            output_dataframe = output_dataframe.append(dataframe, ignore_index=True)\n",
    "    \n",
    "    print('\\n\\n[Success] Content extraction finished.\\n\\n')\n",
    "    \n",
    "    if not save_separately: \n",
    "        return output_dataframe\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "def initialize_driver(mode = 'fast', implicit_wait = 10):\n",
    "    \"\"\"Initialize and return the web driver.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mode: string (optional, default = 'fast')\n",
    "        Loading strategy for the web driver, \"fast\" or \"complete\".\n",
    "    implicit_wait: int (optional, default = 10) # seconds\n",
    "        How many seconds to wait before the driver throw a \"No Such Element Exception\".\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    seleniumwire.webdriver.browser.Chrome\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    global_variables = get_global_variables()\n",
    "    if 'driver' in global_variables.keys():\n",
    "        print('[Error] You have an active driver running, please terminate it before intializing a new one.')\n",
    "        return global_variables['driver']\n",
    "    else:\n",
    "        caps = DesiredCapabilities().CHROME\n",
    "        caps['pageLoadStrategy'] = 'eager' if mode == 'fast' else 'normal' if mode == 'complete' else 'none'\n",
    "        driver = webdriver.Chrome(ChromeDriverManager().install(), desired_capabilities = caps)\n",
    "        driver.implicitly_wait(implicit_wait)\n",
    "        return driver\n",
    "\n",
    "def terminate_driver(driver_instance):\n",
    "    \"\"\"Terminate the web driver and delete the driver variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    driver: seleniumwire.webdriver.browser.Chrome\n",
    "        An active web driver instance.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        if driver_instance is not None:\n",
    "            driver_instance.quit()\n",
    "        global driver\n",
    "        del driver\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print('[Error] Web driver failed to quit properly. '+str(err))\n",
    "        return False\n",
    "        \n",
    "def go_to_page(driver, url):\n",
    "    \"\"\"Initialize and return the web driver.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    implicit_wait: int (optional, default = 10) # seconds\n",
    "        How many seconds to wait before the driver throw a \"No Such Element Exception\" \n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    seleniumwire.webdriver.browser.Chrome\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        return True\n",
    "    except Exception as err:\n",
    "        print('[Error] '+str(err))\n",
    "        return False\n",
    "\n",
    "def get_page_source(driver):\n",
    "    \"\"\"Return the page source of the web page that the driver is currently at.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    driver: seleniumwire.webdriver.browser.Chrome\n",
    "        An active web driver instance.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    return driver.page_source\n",
    "\n",
    "def is_driver_at_url(driver, url):\n",
    "    \"\"\"Return whether the driver is at the url provided.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    driver: seleniumwire.webdriver.browser.Chrome\n",
    "        An active web driver instance.\n",
    "    url: string\n",
    "        A url to check if the driver is at.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "    stripped_url = re.sub('https?://','', url.strip('/'))\n",
    "    stripped_current_url = re.sub('https?://','', driver.current_url.strip('/'))\n",
    "    return stripped_current_url == stripped_url\n",
    "\n",
    "def scroll_to_bottom(driver, scroll_pause_time = 1):\n",
    "    \"\"\"Make the driver scroll to the bottom of the page step by step by a certain pause interval.\n",
    "    # Reference: https://stackoverflow.com/a/28928684/1316860\n",
    "    # Reference: https://stackoverflow.com/a/43299513\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    driver: seleniumwire.webdriver.browser.Chrome\n",
    "        An active web driver instance.\n",
    "    scroll_pause_time: int (optional, default = 1)\n",
    "        Time to wait between scroll actions.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    boolean\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    # Get scroll height\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        # Wait to load page\n",
    "        time.sleep(scroll_pause_time)\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "driver = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell if you want to scrape dynamic websites.\n",
    "\n",
    "if 'driver' in get_global_variables().keys():\n",
    "    terminate_driver(driver)\n",
    "    \n",
    "driver = initialize_driver(mode = 'complete') # choose 'fast' or 'complete' depending on your need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://digitalcollections.nypl.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"Children's Book\", \"https://digitalcollections.nypl.org\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"The Black Experience in Children's Books\",\"https://digitalcollections.nypl.org\", go_up = 1)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://digitalcollections.nypl.org/collections/changing-new-york"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"broome street\", \"https://digitalcollections.nypl.org/collections/changing-new-york\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"Salmagundi Club\", \"https://digitalcollections.nypl.org/collections/changing-new-york\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7\n",
    "\n",
    "This website has active anti-scraping measures in place, requires disguising the Selenium driver. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, path = scrape_what_from_where(\"2013-12\", \"https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7\", driver)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_response_and_save_html(\"https://www.aqistudy.cn/historydata/monthdata.php?city=%E4%B8%8A%E6%B5%B7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"清上海书业商\", \"http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index.html\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagination Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = create_page_url_list(template_url = 'http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index_NUMBER.html?tm=1579785391762', start_index = 1, end_index = 4, unique_first_url = 'http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = scrape_path_from_pages(path, pages, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = get_base_url('http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/index_NUMBER.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['url'] = base_url + df['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_pages = df['url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/2020/01/23/3419955b6e5e35b7016fd28a7f890c42.html?tm=1579785391841'\n",
    "data, info_path = scrape_what_from_where('革命期间发挥了', url, driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://www.shh-shrhmuseum.org.cn/historymuseum/historymuseum/dc/myyp/2020/01/23/3419955b6e5e35b7016fd28a7f890c42.html?tm=1579785391841'\n",
    "data, info_path = scrape_what_from_where('革命期间发挥了', url, driver, go_up = 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = scrape_path_from_pages(info_path, info_pages, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df.text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, info_path = scrape_what_from_where(\"限大额\", \"http://fund.eastmoney.com/fund.html#os_0;isall_0;ft_;pt_1\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.nyc.com/arts__attractions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"Central Park\", \"https://www.nyc.com/arts__attractions/\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"ellis island museum\", \"https://www.nyc.com/arts__attractions/\", driver, go_up = 3)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://search.huochepiao.com/chezhan/shanghai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where(\"上海 - 南京\", \"http://search.huochepiao.com/chezhan/shanghai\", driver)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = get_base_url(\"http://search.huochepiao.com/chezhan/shanghai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['url'] = base_url + data['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = data['url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where('G4824', pages[0], driver, go_up = 3)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schedule_df = scrape_path_from_pages(path, pages, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_schedule_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.broadway.com/shows/tickets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where('to kill a mockingbird', 'https://www.broadway.com/shows/tickets/', driver)\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data, path = scrape_what_from_where('to kill a mockingbird', 'https://www.broadway.com/shows/tickets/', driver, go_up = 2)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = create_page_url_list(template_url = 'https://www.broadway.com/shows/tickets/?page=NUMBER', start_index = 1, end_index = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broadway_show_df = scrape_path_from_pages(path, pages, driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "broadway_show_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To-do and notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ########################################################################\n",
    "\n",
    "# def process_data_request(data_request):\n",
    "#     try:\n",
    "#         return data_request.url, ast.literal_eval(re.findall(r'\\((.*?)\\)',data_request.response.body.decode(\"utf-8\"))[0])['data']\n",
    "#     except:\n",
    "#         return ('',{})\n",
    "\n",
    "# raw_data_requests = []\n",
    "# for request in driver.requests:\n",
    "#     if request.response:\n",
    "#         if \"get\" in request.path: # True\n",
    "#             raw_data_requests.append(request)\n",
    "\n",
    "# ### data_requests\n",
    "\n",
    "# data_requests = [process_data_request(x) for x in raw_data_requests if process_data_request(x)[0]!='']\n",
    "\n",
    "# ########################################################################\n",
    "\n",
    "# ########################################################################\n",
    "# # # Download file without asking to a specified path\n",
    "# # # Reference: https://stackoverflow.com/a/57511043\n",
    "# # # For chrome:\n",
    "# from selenium import webdriver\n",
    "# options = webdriver.ChromeOptions()\n",
    "# options.add_argument(\"--start-maximized\")\n",
    "# prefs = {\"profile.default_content_settings.popups\": 0,\n",
    "#              \"download.default_directory\": r\"C:\\Users\\user_dir\\Desktop\\\\\",#IMPORTANT - ENDING SLASH VERY IMPORTANT\n",
    "#              \"directory_upgrade\": True}\n",
    "# options.add_experimental_option(\"prefs\", prefs)\n",
    "# browser=webdriver.Chrome(<chromdriver.exe path>, options=options)\n",
    "# ########################################################################\n",
    "\n",
    "# ########################################################################\n",
    "# # # Set request header\n",
    "# # # Reference: https://stackoverflow.com/a/51919307\n",
    "\n",
    "# from seleniumwire import webdriver  # Import from seleniumwire\n",
    "\n",
    "# # Create a new instance of the Firefox driver (or Chrome)\n",
    "# driver = webdriver.Firefox()\n",
    "\n",
    "# # Create a request interceptor\n",
    "# def interceptor(request):\n",
    "#     del request.headers['Referer']  # Delete the header first\n",
    "#     request.headers['Referer'] = 'some_referer'\n",
    "\n",
    "# # Set the interceptor on the driver\n",
    "# driver.request_interceptor = interceptor\n",
    "\n",
    "# # All requests will now use 'some_referer' for the referer\n",
    "# driver.get('https://mysite')\n",
    "# ########################################################################\n",
    "\n",
    "# # Reference: https://stackoverflow.com/questions/33225947/can-a-website-detect-when-you-are-using-selenium-with-chromedriver"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
